<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NLP Tasks & Applications - Neural NLP Fundamentals</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUbGuHTCQ" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="icon" type="image/png" href="logo.png">
    <style>
        body { font-family: 'Inter', sans-serif; }
        .katex-display { display: block; margin: 1em 0; text-align: center; }
        .content-section h1 { margin-bottom: 1.5rem; font-size: 2.25rem; font-weight: 700; color: #4f46e5; /* Indigo-600 */ }
        .content-section h2 { margin-top: 2rem; margin-bottom: 1rem; font-size: 1.75rem; font-weight: 600; border-bottom: 2px solid #e5e7eb; padding-bottom: 0.5rem; }
        .content-section h3 { margin-top: 1.5rem; margin-bottom: 0.75rem; font-size: 1.35rem; font-weight: 600; }
        .content-section h4 { margin-top: 1.25rem; margin-bottom: 0.5rem; font-size: 1.15rem; font-weight: 600; }
        .content-section p, .content-section ul, .content-section ol { margin-bottom: 1rem; line-height: 1.65; color: #374151; /* Gray-700 */ }
        .content-section ul { list-style-type: disc; margin-left: 1.5rem; }
        .content-section ol { list-style-type: decimal; margin-left: 1.5rem; }
        .content-section code { background-color: #f3f4f6; padding: 0.2em 0.4em; margin: 0; font-size: 85%; border-radius: 3px; color: #4b5563; /* Gray-600 */ }
        .content-section pre { background-color: #f9fafb; /* Gray-50 */ border: 1px solid #e5e7eb; /* Gray-200 */ padding: 1em; border-radius: 0.375rem; /* rounded-md */ overflow-x: auto; margin-bottom: 1rem; }
        .content-section pre code { background-color: transparent; padding: 0; font-size: 90%; }
        .nav-link { padding: 0.5rem 1rem; border-radius: 0.375rem; transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out; }
        .nav-link:hover { background-color: #e0e7ff; /* Indigo-100 */ color: #3730a3; /* Indigo-700 */ }
        .nav-link.active { background-color: #4f46e5; /* Indigo-600 */ color: white; }
        .note { background-color: #f0fdf4; /* Green-50 */ border-left: 4px solid #22c55e; /* Green-500 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.25rem;}
        .note strong { color: #15803d; /* Green-700 */ }
        .example-box { background-color: #eef2ff; /* Indigo-50 */ border: 1px solid #c7d2fe; /* Indigo-200 */ border-left-width: 4px; border-left-color: #6366f1; /* Indigo-500 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.375rem; }
        .example-box h5 { font-weight: 600; color: #4338ca; /* Indigo-700 */ margin-bottom: 0.5rem; }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">

    <nav class="bg-white shadow-md sticky top-0 z-50">
        <div class="container mx-auto px-6 py-3 flex flex-wrap justify-between items-center">
            <a href="index.html" class="flex items-center text-xl font-bold text-blue-600">
                <img src="logo.png" alt="NLP Fundamentals Logo" class="h-8 w-auto mr-2">
                <span>NLP Fundamentals</span>
            </a>
            <button id="mobile-menu-button" class="md:hidden text-gray-600 hover:text-gray-800 focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>
            <div id="mobile-menu" class="w-full md:w-auto md:flex md:flex-nowrap md:overflow-x-auto space-y-2 md:space-y-0 md:space-x-1 hidden mt-3 md:mt-0">
                <a href="index.html" class="nav-link text-gray-700 block md:inline-block">Home</a>
                <a href="text_processing.html" class="nav-link text-gray-700 block md:inline-block">Text Processing</a>
                <a href="regex.html" class="nav-link text-gray-700 block md:inline-block">Regex</a>
                <a href="language_models.html" class="nav-link text-gray-700 block md:inline-block">Language Models</a>
                <a href="sequence_labelling.html" class="nav-link text-gray-700 block md:inline-block">Sequence Labelling</a>
                <a href="constituency_grammars.html" class="nav-link text-gray-700 block md:inline-block">Constituency Grammars</a>
                <a href="sparse_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Sparse Embeddings</a>
                <a href="word_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Word Embeddings (Neural)</a>
                <a href="recurrent_neural_networks.html" class="nav-link text-gray-700 block md:inline-block">RNNs</a>
                <a href="seq2seq_attention.html" class="nav-link text-gray-700 block md:inline-block">Seq2Seq & Attention</a>
                <a href="transformer_architecture.html" class="nav-link text-gray-700 block md:inline-block">Transformers</a>
                <a href="transformer_models_pretraining.html" class="nav-link text-gray-700 block md:inline-block">Transformer Models</a>
                <a href="finetuning_advanced_llm.html" class="nav-link text-gray-700 block md:inline-block">Fine-tuning LLMs</a>
                <a href="nlp_tasks_applications.html" class="nav-link text-gray-700 active block md:inline-block">NLP Tasks</a>
                <a href="info_extraction.html" class="nav-link text-gray-700 block md:inline-block">Information Extraction</a>
                <a href="evaluation_metrics_nlp.html" class="nav-link text-gray-700 block md:inline-block">Evaluation Metrics</a>
                <a href="lab_regex.html" class="nav-link text-gray-700 active block md:inline-block">Regex Lab</a>
                <a href="lab_crf.html" class="nav-link text-gray-700 block md:inline-block">CRF Lab</a>
                <a href="lab_bert.html" class="nav-link text-gray-700 block md:inline-block">BERT Lab</a>
                <a href="lab_llama.html" class="nav-link text-gray-700 block md:inline-block">Llama Lab</a>
            </div>
        </div>
    </nav>

    <main class="container mx-auto px-6 py-12 content-section">
        <h1>Part 3: NLP Tasks (Applications and Techniques)</h1>
        <p>
            This part delves into specific Natural Language Processing tasks, outlining their goals,
            challenges, and how the foundational and neural techniques discussed earlier are applied.
        </p>

        <section id="qa">
            <h2>Section 3.1: Question Answering (QA)</h2>
            <p><strong>Primary Goal:</strong> To enable systems to answer questions posed in natural language.</p>
            <p><strong>Challenges:</strong> Understanding the question's intent, locating relevant information (often in large text corpora or knowledge bases), and synthesizing a coherent, accurate answer. Types include extractive QA (answer is a span in context) and abstractive QA (answer is generated).</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>BERT Fine-tuning (SQuAD-style):</strong> For extractive QA, BERT is fine-tuned by adding layers to predict the start and end tokens of the answer span within a given context passage. The <code>[CLS]</code> token isn't typically used for span prediction here.</li>
                <li><strong>SpanBERT:</strong> Its span-masking and Span Boundary Objective (SBO) make it particularly effective for extractive QA tasks where identifying precise answer spans is crucial.</li>
                <li><strong>Retrieval-Augmented Generation (RAG):</strong> Combines a retriever (to find relevant documents) with a generator (LLM) to answer questions, especially useful for open-domain QA where knowledge might not be in the LLM's parameters.</li>
                <li><strong>Generate-then-Read (GENREAD):</strong> An LLM first generates relevant context, then a reader model answers based on this generated context.</li>
            </ul>
            <div class="example-box">
                <h5>Simplified Example: Extractive QA</h5>
                <p><strong>Context:</strong> "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower."</p>
                <p><strong>Question:</strong> "Who is the Eiffel Tower named after?"</p>
                <p><strong>Answer (Span):</strong> "Gustave Eiffel"</p>
            </div>
        </section>

        <section id="ner">
            <h2>Section 3.2: Named Entity Recognition (NER)</h2>
            <p><strong>Primary Goal:</strong> To identify and classify named entities in text into predefined categories (e.g., PERSON, ORGANIZATION, LOCATION, DATE).</p>
            <p><strong>Challenges:</strong> Ambiguity (e.g., "Washington"), multi-token entities ("Stanford University"), new entities not seen in training.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>BERT Fine-tuning:</strong> A token classification head is added on top of BERT's final hidden states for each token. The model is trained to predict BIO/BIOES tags for each token.</li>
                <li>CRFs are often used on top of BiLSTMs or BERT outputs to model dependencies between tags.</li>
            </ul>
        </section>

        <section id="srl">
            <h2>Section 3.3: Semantic Role Labeling (SRL)</h2>
            <p><strong>Primary Goal:</strong> To identify the semantic roles of constituents in a sentence with respect to a predicate (typically a verb). Answers "who did what to whom, when, where, why, how".</p>
            <p><strong>Challenges:</strong> Identifying predicates, disambiguating roles, handling complex sentence structures, accounting for long-distance dependencies, and resolving structural ambiguities (e.g., PP-attachment errors, adjunct-argument distinctions).</p>
            
            <p>Semantic Role Labeling is a crucial step toward natural language understanding, as it reveals the underlying predicate-argument structure of sentences. SRL has evolved from requiring extensive syntactic parsing and feature engineering to leveraging deep learning architectures that can learn meaningful representations directly from data. This progression reflects the broader trend in NLP from heavily engineered systems to data-driven approaches.</p>
            
            <h4>Key Architectures and Techniques:</h4>
            
            <div class="example-box">
                <h5>Deep Highway BiLSTM with Constrained Decoding (He et al., 2017)</h5>
                <p><strong>Core Idea:</strong> Use deep bidirectional LSTMs with highway connections to create rich contextual representations, combined with constrained decoding to enforce structural consistency in predictions.</p>
                <p><strong>Architecture:</strong></p>
                <ul>
                    <li><strong>Deep BiLSTM Encoder:</strong> 8-layer bidirectional LSTM with highway connections between layers, providing more direct paths for gradient flow during training.</li>
                    <li><strong>BIO Tagging Approach:</strong> Treats SRL as a sequence tagging problem, where each token receives a tag indicating whether it's the Beginning, Inside, or Outside of an argument with a specific role.</li>
                    <li><strong>Model Optimizations:</strong> Incorporates recurrent dropout and orthonormal initialization to improve training stability and performance.</li>
                    <li><strong>Constrained Decoding:</strong> Applies A* search over tag prefixes during inference to enforce constraints like valid BIO transitions, unique core roles, and continuation roles, without adding complexity to training.</li>
                    <li><strong>Syntax Relationship:</strong> While not using syntax as direct input, the model's predictions showed high consistency with gold syntactic structures (94.3% agreement with unlabeled constituents).</li>
                </ul>
                <p><strong>Results:</strong> Achieved significant improvements over previous state-of-the-art (83.2 F1 on CoNLL 2005 test set and 83.4 F1 on CoNLL 2012), representing approximately a 10% relative error reduction. Analysis showed deep models excel at capturing long-distance dependencies, but still struggled with certain linguistic phenomena.</p>
                <p><strong>Significance:</strong> Demonstrated that powerful deep learning models could achieve strong SRL performance without explicit syntactic features, challenging the long-held belief that syntax was a prerequisite for SRL. However, experiments also showed that high-quality syntactic information could still provide significant improvements (up to 2 F1 points with gold syntax).</p>
            </div>
            
            <div class="example-box">
                <h5>Simple BERT Models for SRL (Shi & Lin, 2019)</h5>
                <p><strong>Core Idea:</strong> Leverage powerful pretrained contextual embeddings from BERT to achieve state-of-the-art SRL performance with minimal architectural complexity and without external linguistic features.</p>
                <p><strong>Architecture:</strong></p>
                <ul>
                    <li><strong>Input Representation:</strong> The sentence along with a predicate indicator (marking the target predicate for which arguments need to be identified) fed into BERT encoder.</li>
                    <li><strong>Simple Classification Layer:</strong> For predicate sense disambiguation, a simple MLP classifier on top of BERT's representation for the predicate token.</li>
                    <li><strong>Argument Identification & Classification:</strong> BERT contextual representations combined with predicate indicator embeddings, fed into a lightweight one-layer BiLSTM, followed by an MLP classifier predicting BIO tags.</li>
                    <li><strong>Feature Integration:</strong> Concatenation of the current token's hidden state and the predicate's hidden state before final classification, creating a direct link between arguments and their associated predicate.</li>
                    <li><strong>Notable Simplifications:</strong> No external lexical or syntactic features, no complex hand-crafted constraints or rules, significantly simpler than previous state-of-the-art systems.</li>
                </ul>
                <p><strong>Results:</strong> The BERT-LSTM-large model achieved state-of-the-art performance across multiple benchmarks: better accuracy than previous models using linguistic features on predicate disambiguation (CoNLL 2009), significant F1 improvements on dependency-based SRL (CoNLL 2009), and state-of-the-art F1 among single models on span-based SRL (CoNLL 2005).</p>
                <p><strong>Significance:</strong> Demonstrated that powerful pretrained language models like BERT fundamentally change the landscape for SRL, allowing simpler architectures to achieve or surpass previous state-of-the-art results that relied on extensive feature engineering and complex decoding constraints. This represents a shift from engineering task-specific features to effectively leveraging general-purpose linguistic understanding encoded in large pretrained models.</p>
            </div>
            
            <h4>Evolution and Trends:</h4>
            <p>The progression from deep, carefully engineered BiLSTM architectures with constrained decoding (He et al., 2017) to simpler BERT-based models (Shi & Lin, 2019) illustrates the transformative impact of large pretrained language models on SRL:</p>
            <ol>
                <li><strong>Decreasing Reliance on Syntax:</strong> While earlier approaches considered syntactic parsing a prerequisite for SRL, modern neural approaches have demonstrated strong performance without explicit syntactic input, though high-quality syntax may still offer marginal benefits.</li>
                <li><strong>Architectural Simplification:</strong> The trend moved from complex, task-specific architectures to leveraging the rich contextual representations from pretrained models with minimal additional components.</li>
                <li><strong>Trade-offs in Constraint Enforcement:</strong> While BERT-based approaches rely less on explicit constraints, certain complex decoding mechanisms with specific constraints may still offer advantages for particular metrics or datasets (e.g., higher precision on CoNLL 2012).</li>
                <li><strong>Persistent Challenges:</strong> Despite significant advances, models still struggle with certain linguistic phenomena like PP-attachment errors, adjunct-argument distinctions, and structural ambiguities in complex sentences.</li>
            </ol>
            <p>These developments suggest that while simpler BERT-based models provide strong SRL baselines, there remains potential for further improvements through carefully integrating linguistic knowledge or task-specific constraints in ways that complement rather than compete with the strengths of large pretrained models.</p>
        </section>

        <section id="wsd">
            <h2>Section 3.4: Word Sense Disambiguation (WSD)</h2>
            <p><strong>Primary Goal:</strong> To identify the correct meaning (sense) of a word in a given context, especially for polysemous words (words with multiple meanings).</p>
            <p><strong>Challenges:</strong> Fine-grained distinctions between senses, lack of large-scale sense-annotated data for all words/languages, data sparsity with rare senses (the "long tail" problem), and language-specific characteristics that affect word meaning.</p>
            
            <p>Word Sense Disambiguation is fundamental to many NLP applications, as resolving lexical ambiguity improves performance in tasks like machine translation and information extraction. Historically, WSD relied on manually engineered features or statistical techniques, but the rise of contextual embeddings from Neural Language Models (NLMs) has transformed the field by providing context-sensitive word representations.</p>
            
            <h4>Key Architectures and Techniques:</h4>
            
            <div class="example-box">
                <h5>Language Model Makes Sense (LMMS) - Full-Coverage WSD (Loureiro & Jorge, 2019)</h5>
                <p><strong>Core Idea:</strong> Represent every possible sense in WordNet as a dense vector embedding and perform WSD by finding the sense embedding closest to the contextual embedding of the target word.</p>
                <p><strong>Architecture:</strong></p>
                <ul>
                    <li><strong>Bootstrapping Sense Embeddings:</strong> Start with sense-annotated corpora (like SemCor) to create initial sense embeddings by averaging contextual embeddings from ELMo or BERT for each sense.</li>
                    <li><strong>WordNet Propagation:</strong> Extend coverage to all ~200K WordNet senses by leveraging the hierarchical structure (synonyms, hypernyms, lexnames) to impute embeddings for senses missing in annotated data.</li>
                    <li><strong>Dictionary Definitions:</strong> Incorporate gloss embeddings by averaging contextual embeddings of words in dictionary definitions, making them sense-specific by including the target lemma.</li>
                    <li><strong>Combined Representation:</strong> Concatenate bootstrapped sense embeddings with dictionary embeddings and static word embeddings (for morphological robustness).</li>
                    <li><strong>k-NN Disambiguation:</strong> For a word in context, generate its contextual embedding and find the closest sense embedding using cosine similarity (1-NN).</li>
                </ul>
                <p><strong>Significance:</strong> Achieved state-of-the-art results without relying on Most Frequent Sense (MFS) fallbacks, demonstrating the power of dense sense representations aligned with contextual embeddings. The method also provides "full-coverage" of all WordNet senses.</p>
            </div>
            
            <div class="example-box">
                <h5>Gloss-Informed Bi-encoders (BEM) - Tackling Rare Senses (Blevins & Zettlemoyer, 2020)</h5>
                <p><strong>Core Idea:</strong> Learn to embed both the target word in context and sense definitions (glosses) into the same vector space, addressing the "long tail" problem of rare and unseen senses.</p>
                <p><strong>Architecture:</strong></p>
                <ul>
                    <li><strong>Dual Encoders:</strong> Two independent transformer encoders initialized with pretrained BERT:
                        <ul>
                            <li><strong>Context Encoder (Tc):</strong> Processes the sentence with the target word, producing a representation for the target.</li>
                            <li><strong>Gloss Encoder (Tg):</strong> Processes the dictionary definition of a sense, using the [CLS] token embedding as the sense representation.</li>
                        </ul>
                    </li>
                    <li><strong>Joint Training:</strong> Both encoders are jointly optimized using cross-entropy loss based on dot-product similarity between context and sense embeddings.</li>
                    <li><strong>WSD Process:</strong> Compute embeddings for the target word in context and all its possible senses (via their definitions), then select the sense with highest similarity score.</li>
                </ul>
                <p><strong>Significance:</strong> Significantly improved performance on less frequent senses (LFS) and zero-shot WSD compared to previous approaches. Demonstrated that directly modeling sense definitions and aligning them with contextual word representations is particularly effective for rare senses. Showed strong data efficiency in few-shot settings.</p>
            </div>
            
            <div class="example-box">
                <h5>FormBERT - Word-Formation Knowledge for Chinese WSD (Zheng et al., 2021)</h5>
                <p><strong>Core Idea:</strong> Incorporate word-formation knowledge (how constituent characters form words in Chinese) to improve WSD performance, as these formations often provide crucial clues about word meaning.</p>
                <p><strong>Architecture:</strong></p>
                <ul>
                    <li><strong>Foundation:</strong> BERT-based binary classifier that takes concatenated context and sense definition as input.</li>
                    <li><strong>Word-Formation Integration:</strong> Add a learned embedding for the word-formation type (e.g., Modifier-Head, Verb-Object) to the BERT output before classification.</li>
                    <li><strong>Word-Formation Predictor (FP):</strong> An auxiliary module (typically an MLP) that predicts the word-formation from context when annotations aren't available, enabling broader application.</li>
                    <li><strong>Training Objective:</strong> Combined loss function with the primary WSD classification loss and an auxiliary loss for word-formation prediction.</li>
                </ul>
                <p><strong>Significance:</strong> First work to explicitly leverage word-formation knowledge for Chinese WSD. Created a large-scale, formation-annotated dataset (FiCLS) for Chinese NLP research. Demonstrated substantial performance improvements over BERT baselines by incorporating this language-specific feature.</p>
            </div>
            
            <h4>Evolution and Trends:</h4>
            <p>These approaches represent an evolution in WSD research, building upon contextual embeddings from pretrained language models:</p>
            <ol>
                <li><strong>Leveraging Embeddings and External Knowledge (LMMS, 2019):</strong> Using contextual embeddings with structured knowledge (WordNet) and glosses to create comprehensive sense inventories.</li>
                <li><strong>Targeting Data Imbalance (BEM, 2020):</strong> Directly addressing the skewed distribution of sense occurrences through dedicated bi-encoder architectures.</li>
                <li><strong>Incorporating Language-Specific Features (FormBERT, 2021):</strong> Augmenting pretrained models with explicit linguistic knowledge specific to the target language.</li>
            </ol>
            <p>All three approaches demonstrate that while large pretrained models provide powerful general representations, enhancing them with clever architectural designs and linguistic knowledge leads to significant improvements in WSD performance.</p>
        </section>

        <section id="re">
            <h2>Section 3.5: Relation Extraction (RE)</h2>
            <p><strong>Primary Goal:</strong> To identify and classify semantic relationships between entities in text (e.g., "Steve Jobs" - Founded - "Apple Inc.").</p>
            <p><strong>Challenges:</strong> Distinguishing between different relation types, handling long-distance relations, dealing with sentences expressing multiple relations.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>LSTMs with Position-Aware Attention:</strong> LSTMs can model sentence context, and position-aware attention helps focus on the relevant parts of the sentence concerning the two entities involved in a potential relation. Position embeddings relative to the target entities are often used.</li>
                <li><strong>BERT-based Models:</strong> Fine-tuned by adding a classification layer over representations of the target entities (or the <code>[CLS]</code> token), often with special markers around entities.</li>
                <li><strong>Open Information Extraction (OpenIE):</strong> Aims to extract relational tuples (e.g., (Arg1, RelationPhrase, Arg2)) from text without a predefined schema or set of relation types. Often uses rule-based or lightly supervised methods.</li>
            </ul>
            <p>For a more detailed exploration of Information Extraction approaches, including Position-aware Attention, SpanBERT, and Open Information Extraction, see our dedicated <a href="info_extraction.html" class="text-purple-600 hover:underline">Information Extraction</a> page.</p>
        </section>

        <section id="text-generation-summarization">
            <h2>Section 3.6: Text Generation and Summarization</h2>
            <p><strong>Primary Goals:</strong> Generating human-like text (e.g., stories, dialogue) or creating concise summaries of longer documents (extractive or abstractive).</p>
            <p><strong>Challenges:</strong> Maintaining coherence, factual accuracy, avoiding repetition, controlling style and content, evaluating quality.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>Transformer-based models (Decoder-only like GPT/Llama, Encoder-Decoder like BART/T5):</strong> These are state-of-the-art.
                    <ul>
                        <li><strong>Autoregressive decoding:</strong> Generating text token by token.</li>
                        <li><strong>Fine-tuning:</strong> Adapting pre-trained models on specific generation/summarization datasets.</li>
                        <li><strong>Prompting strategies:</strong> Using carefully crafted prompts (including few-shot examples) to guide generation.</li>
                        <li>RAG can be used for knowledge-grounded generation/summarization.</li>
                    </ul>
                </li>
            </ul>
        </section>

        <section id="mt">
            <h2>Section 3.7: Machine Translation (MT)</h2>
            <p><strong>Primary Goal:</strong> To automatically translate text from a source language to a target language.</p>
            <p><strong>Challenges:</strong> Lexical ambiguity, syntactic differences between languages, maintaining meaning and fluency, handling idioms and cultural nuances.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>Encoder-Decoder Transformers:</strong> The original Transformer paper ("Attention Is All You Need") demonstrated its effectiveness for MT. The encoder processes the source sentence, and the decoder generates the target sentence, with attention mechanisms linking target words to relevant source words.</li>
            </ul>
        </section>

        <section id="nli">
            <h2>Section 3.8: Natural Language Inference (NLI) / Textual Entailment</h2>
            <p><strong>Primary Goal:</strong> To determine the relationship between a pair of sentences (premise and hypothesis): entailment, contradiction, or neutral.</p>
            <p><strong>Challenges:</strong> Requires understanding nuanced meaning, commonsense reasoning, and logical inference.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>BERT Fine-tuning:</strong> Input is formatted as <code>[CLS] premise [SEP] hypothesis [SEP]</code>. The final hidden state of the <code>[CLS]</code> token is fed to a classifier to predict the relationship.</li>
            </ul>
        </section>

        <section id="dialogue-systems">
            <h2>Section 3.9: Dialogue Systems</h2>
            <p><strong>Primary Goal:</strong> To enable machines to converse naturally with humans (task-oriented or open-domain chatbots).</p>
            <p><strong>Challenges:</strong> Maintaining context over multiple turns, understanding user intent, generating coherent and relevant responses, managing dialogue state, ensuring safety and helpfulness.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>Fine-tuning LLMs like Llama 2-Chat:</strong> Involves Supervised Fine-Tuning (SFT) on dialogue data, Reinforcement Learning from Human Feedback (RLHF) to align with preferences (helpfulness, safety), and techniques like Ghost Attention (GAtt) for multi-turn consistency.</li>
            </ul>
        </section>

        <section id="fact-checking">
            <h2>Section 3.10: Fact Checking</h2>
            <p><strong>Primary Goal:</strong> To verify the factual accuracy of claims against evidence from reliable sources.</p>
            <p><strong>Challenges:</strong> Identifying check-worthy claims, retrieving relevant evidence, reasoning over evidence to determine veracity.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li>Potential application of <strong>RAG</strong> (to retrieve evidence and generate a verdict) or <strong>GENREAD</strong> (to generate supporting/refuting arguments based on internal knowledge before making a judgment). NLI techniques are also relevant for comparing claims to evidence.</li>
            </ul>
        </section>

        <section id="coreference-resolution">
            <h2>Section 3.11: Coreference Resolution</h2>
            <p><strong>Primary Goal:</strong> To identify all expressions in a text that refer to the same real-world entity (e.g., linking pronouns to their antecedents).</p>
            <p><strong>Challenges:</strong> Ambiguity, long-distance coreference, distinguishing between different entities with similar mentions.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>SpanBERT:</strong> Its design focusing on span representations makes it well-suited for coreference resolution, which involves identifying and linking spans of text (mentions).</li>
            </ul>
        </section>

        <section id="kbp">
            <h2>Section 3.12: Slot Filling (Knowledge Base Population - KBP)</h2>
            <p><strong>Primary Goal:</strong> To extract specific attributes (slots) for given entities from text and use them to populate a knowledge base (e.g., finding the "birthdate" and "spouse" for a "PERSON" entity).</p>
            <p><strong>Challenges:</strong> Identifying correct slot values, handling variations in how information is expressed, distinguishing between multiple potential fillers.</p>
            <h4>Techniques & Applications:</h4>
            <ul>
                <li><strong>LSTMs with Position-Aware Attention:</strong> Used to identify slot fillers for an entity, where position-aware attention helps focus on relevant parts of the sentence given the entity and the target slot. Supervised data with labeled slot fills is crucial.</li>
                <li>BERT-based models can also be fine-tuned for this, often formulated as a sequence labeling or question answering task.</li>
            </ul>
        </section>

    </main>

    <footer class="bg-gray-800 text-white py-8 mt-12">
        <div class="container mx-auto px-6 text-center">
            <p>&copy; <span id="currentYear"></span> NLP Fundamentals. For educational purposes.</p>
            <p class="text-sm text-gray-400">Content derived from "NLP Exam Preparation" notes.</p>
        </div>
    </footer>

    <script>
        // KaTeX auto-render
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ],
                throwOnError : false
            });

            // Set current year in footer
            document.getElementById('currentYear').textContent = new Date().getFullYear();

            // Active Nav Link Highlighting & Mobile Menu Toggle
            const currentLocation = window.location.pathname.split('/').pop() || 'index.html';
            const navLinks = document.querySelectorAll('nav a.nav-link');
            navLinks.forEach(link => {
                if (link.getAttribute('href') === currentLocation) {
                    link.classList.add('active');
                    link.classList.remove('text-gray-700');
                } else {
                    link.classList.remove('active');
                    link.classList.add('text-gray-700');
                }
            });

            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileMenu = document.getElementById('mobile-menu');
            if (mobileMenuButton && mobileMenu) {
                mobileMenuButton.addEventListener('click', function() {
                    mobileMenu.classList.toggle('hidden');
                });
            }
        });
    </script>

</body>
</html>
