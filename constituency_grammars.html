<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture: Constituency Grammars - NLP Fundamentals</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUbGuHTCQ" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; }
        .katex-display { display: block; margin: 1em 0; text-align: center; }
        .content-section h1 { margin-bottom: 1.5rem; font-size: 2.25rem; font-weight: 700; color: #0d9488; /* Teal-600 chosen for this new page */ }
        .content-section h2 { margin-top: 2rem; margin-bottom: 1rem; font-size: 1.75rem; font-weight: 600; border-bottom: 2px solid #e5e7eb; padding-bottom: 0.5rem; }
        .content-section h3 { margin-top: 1.5rem; margin-bottom: 0.75rem; font-size: 1.35rem; font-weight: 600; }
        .content-section h4 { margin-top: 1.25rem; margin-bottom: 0.5rem; font-size: 1.15rem; font-weight: 600; }
        .content-section p, .content-section ul, .content-section ol { margin-bottom: 1rem; line-height: 1.65; color: #374151; /* Gray-700 */ }
        .content-section ul { list-style-type: disc; margin-left: 1.5rem; }
        .content-section ol { list-style-type: decimal; margin-left: 1.5rem; }
        .content-section code { background-color: #f3f4f6; padding: 0.2em 0.4em; margin: 0; font-size: 85%; border-radius: 3px; color: #4b5563; /* Gray-600 */ }
        .content-section pre { background-color: #f0fdfa; /* Teal-50 */ border: 1px solid #ccfbf1; /* Teal-200 */ padding: 1em; border-radius: 0.375rem; /* rounded-md */ overflow-x: auto; margin-bottom: 1rem; }
        .content-section pre code { background-color: transparent; padding: 0; font-size: 90%; color: #134e4a; /* Teal-800 */ }
        .content-section table { width: 100%; margin-bottom: 1rem; border-collapse: collapse; box-shadow: 0 1px 3px 0 rgba(0,0,0,.1), 0 1px 2px 0 rgba(0,0,0,.06); border-radius: 0.5rem; overflow: hidden;}
        .content-section th, .content-section td { border: 1px solid #e5e7eb; padding: 0.75rem 1rem; text-align: left; font-size: 0.875rem; }
        .content-section th { background-color: #f0fdfa; /* Teal-50 */ font-weight: 600; color: #0f766e; /* Teal-700 */ }
        .content-section tr:nth-child(even) { background-color: #f0fdfa; }
        .nav-link { padding: 0.5rem 1rem; border-radius: 0.375rem; transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out; }
        .nav-link:hover { background-color: #ccfbf1; /* Teal-100 */ color: #0f766e; /* Teal-700 */ }
        .nav-link.active { background-color: #0d9488; /* Teal-600 */ color: white; }
        .note { background-color: #eef2ff; /* Indigo-50 */ border-left: 4px solid #6366f1; /* Indigo-500 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.25rem;}
        .note strong { color: #4f46e5; /* Indigo-600 */ }
        .example-box { background-color: #f0fdfa; /* Teal-50 */ border: 1px solid #99f6e4; /* Teal-200 */ border-left-width: 4px; border-left-color: #2dd4bf; /* Teal-400 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.375rem; }
        .example-box h5 { font-weight: 600; color: #0f766e; /* Teal-700 */ margin-bottom: 0.5rem; }
        .formula-box { background-color: #f3f4f6; padding: 1rem; border-radius: 0.375rem; margin-bottom:1rem; text-align:left; }
        .figure-caption { text-align: center; font-style: italic; margin-top: 0.5rem; color: #555; }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">

    <nav class="bg-white shadow-md sticky top-0 z-50">
        <div class="container mx-auto px-6 py-3 flex flex-wrap justify-between items-center">
            <a href="index.html" class="text-xl font-bold text-blue-600">NLP Fundamentals</a>
            <button id="mobile-menu-button" class="md:hidden text-gray-600 hover:text-gray-800 focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>
            <div id="mobile-menu" class="w-full md:w-auto md:flex md:flex-nowrap md:overflow-x-auto space-y-2 md:space-y-0 md:space-x-1 hidden mt-3 md:mt-0">
                <a href="index.html" class="nav-link text-gray-700 block md:inline-block">Home</a>
                <a href="text_processing.html" class="nav-link text-gray-700 block md:inline-block">Text Processing</a>
                <a href="regex.html" class="nav-link text-gray-700 block md:inline-block">Regex Foundations</a>
                <a href="language_models.html" class="nav-link text-gray-700 block md:inline-block">Language Models</a>
                <a href="sequence_labelling.html" class="nav-link text-gray-700 block md:inline-block">Sequence Labelling</a>
                <a href="constituency_grammars.html" class="nav-link text-gray-700 active block md:inline-block">Constituency Grammars</a>
                <a href="sparse_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Sparse Embeddings</a>
                <a href="basic_text_processing_foundations.html" class="nav-link text-gray-700 block md:inline-block">Basic Text Processing</a>
                <a href="word_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Word Embeddings (Neural)</a>
                <a href="recurrent_neural_networks.html" class="nav-link text-gray-700 block md:inline-block">RNNs</a>
                <a href="seq2seq_attention.html" class="nav-link text-gray-700 block md:inline-block">Seq2Seq & Attention</a>
                <a href="transformer_architecture.html" class="nav-link text-gray-700 block md:inline-block">Transformers</a>
                <a href="transformer_models_pretraining.html" class="nav-link text-gray-700 block md:inline-block">Transformer Models</a>
                <a href="finetuning_advanced_llm.html" class="nav-link text-gray-700 block md:inline-block">Fine-tuning LLMs</a>
                <a href="nlp_tasks_applications.html" class="nav-link text-gray-700 block md:inline-block">NLP Tasks</a>
                <a href="evaluation_metrics_nlp.html" class="nav-link text-gray-700 block md:inline-block">Evaluation Metrics</a>
                <a href="lab_regex.html" class="nav-link text-gray-700 block md:inline-block">Regex Lab</a>
                <a href="lab_crf.html" class="nav-link text-gray-700 block md:inline-block">CRF Lab</a>
                <a href="lab_bert.html" class="nav-link text-gray-700 block md:inline-block">BERT Lab</a>
                <a href="lab_llama.html" class="nav-link text-gray-700 block md:inline-block">Llama Lab</a>
            </div>
        </div>
    </nav>

    <main class="container mx-auto px-6 py-12 content-section">
        <h1>Section 1.5: Constituency Grammars</h1>
        <p>This lecture explores constituency grammars, focusing on Context-Free Grammars (CFGs) and their application in understanding sentence structure, along with a comparison to Dependency Grammars. [cite: 1]</p>

        <section id="constituency-ambiguity">
            <h2>Constituency and Ambiguity</h2>
            <p>Syntax refers to the way words are arranged together[cite: 4]. A core concept in syntax is <strong>syntactic constituency</strong>, which is the idea that words can be grouped into single units, such as a Noun Phrase (NP)[cite: 4]. These groups, or <strong>constituents</strong>, are identified using evidence from the sentence's context and function as single units within the grammatical structure[cite: 4]. Grammars, sets of rules, are used to encode this evidence[cite: 4].</p>
            <p>Different types of grammars yield different syntactic structures[cite: 4]. Two prominent types are:</p>
            <ul>
                <li><strong>Context-Free Grammar (CFG):</strong> Also known as Phrase-Structure Grammar, CFGs use rules based on phrasal constituents and their structure[cite: 4]. Word order is very important in CFGs, and head terms (the central word of a phrase) are embedded within trees, which can make them harder to find directly[cite: 4, 5].</li>
                <li><strong>Dependency Grammar:</strong> Rules are based on grammatical dependencies between words[cite: 4]. Word order is more flexible in dependency grammars[cite: 5]. The (Head $\rightarrow$ Dependent) structure often approximates the semantic relationship between predicates and their arguments[cite: 5].</li>
            </ul>

            <h3>Ambiguity</h3>
            <p>Assigning a syntactic structure to a sentence is known as syntactic parsing[cite: 6]. The primary challenge in parsing is <strong>ambiguity</strong>[cite: 6].</p>
            <h4>Structural Ambiguity:</h4>
            <p>This occurs when a grammar allows for multiple valid parse trees for a single sentence[cite: 6]. Two common types include:</p>
            <ul>
                <li>
                    <strong>Attachment Ambiguity:</strong> A constituent could be attached to multiple places in a parse tree[cite: 6].
                    <div class="example-box">
                        <h5>Example: Attachment Ambiguity</h5>
                        <p>"We saw the Eiffel Tower flying to Paris" [cite: 6]</p>
                        <p>Possible interpretations (and partial parses):</p>
                        <ul>
                            <li>We saw (NP_SUBJ[the Eiffel Tower]) (VP[flying to Paris]) - <em>The Eiffel Tower was flying.</em></li>
                            <li>(PRO_SUBJ[We]) VP[saw (NP_OBJ [the Eiffel Tower]) (ADV [flying to Paris])] - <em>We were flying to Paris when we saw the Eiffel Tower.</em></li>
                        </ul>
                    </div>
                </li>
                <li>
                    <strong>Coordination Ambiguity:</strong> Phrases can be conjoined in multiple ways[cite: 6].
                    <div class="example-box">
                        <h5>Example: Coordination Ambiguity</h5>
                        <p>"old men and women" [cite: 6]</p>
                        <p>Possible interpretations:</p>
                        <ul>
                            <li>[old [men and women]] - <em>Old men and old women.</em></li>
                            <li>[[old men] and [women]] - <em>Men who are old, and women (of any age).</em></li>
                        </ul>
                    </div>
                </li>
            </ul>
            <p><strong>Syntactic disambiguation</strong> is the process of choosing the correct parse from multiple possibilities[cite: 6].</p>
            <div class="example-box my-8">
                <img src="https://storage.googleapis.com/generativeai-downloads/images/ba1953.jpg" alt="Parse tree for 'I prefer the morning flight through Denver'" class="mx-auto w-full md:w-3/4 lg:w-1/2">
                <p class="figure-caption">Example of a Constituency (CFG) Parse Tree for "I prefer the morning flight through Denver" (based on image similar to [cite: 7]). The image on page 5 of the PDF [cite: 7] shows both a dependency parse (left) and a constituency parse (right) for this sentence.</p>
            </div>
        </section>

        <section id="cfg">
            <h2>Context Free Grammar (CFG)</h2>
            <p>A Context-Free Grammar (CFG) is a model for constituent structure[cite: 8]. It consists of a lexicon (words and symbols) and a set of rules (productions) dictating how these elements are grouped and ordered[cite: 8]. These rules can be hierarchically embedded, allowing one rule to trigger others[cite: 8].</p>
            <p>CFG rules are expressed in a form equivalent to Backus-Naur Form (BNF), a generative metalanguage from the 1960s[cite: 8]. The general form is: "Given &lt;left symbol&gt; generate &lt;right set of symbols&gt;"[cite: 9].</p>
            
            <h4>Example Productions for a Noun Phrase (NP)[cite: 9]:</h4>
            <div class="formula-box">
                <code>
                NP $\rightarrow$ Det Nominal <br>
                NP $\rightarrow$ ProperNoun <br>
                Nominal $\rightarrow$ Noun <br>
                Nominal $\rightarrow$ Nominal Noun <br>
                Det $\rightarrow$ 'a' | 'the' <br>
                Noun $\rightarrow$ 'flight'
                </code>
            </div>
            <p>A sequence of rule applications is a derivation, like NP $\rightarrow$ Det Nominal $\rightarrow$ Det Noun $\rightarrow$ 'a' 'flight'[cite: 9]. Derivations are commonly shown as parse trees[cite: 9].</p>
            <ul>
                <li><strong>Leaf nodes:</strong> Terminal symbols (words from the lexicon)[cite: 9].</li>
                <li><strong>Non-terminal nodes:</strong> Define lexical categories (POS tags) or phrasal categories[cite: 9].</li>
                <li>A node <strong>dominates</strong> its child nodes[cite: 9].</li>
                <li>The <strong>root node</strong> is the start symbol, usually 'S' (sentence)[cite: 9].</li>
            </ul>

            <div class="example-box">
                <h5>Simplified Parse Tree for "a flight" [cite: 9]</h5>
                <pre><code>
      NP
     /  \
   Det  Nominal
    |     |
   'a'  Noun
          |
       'flight'
                </code></pre>
            </div>

            <h4>Example CFG for Flight Information [cite: 10, 11, 12, 13]</h4>
            <p>A CFG for discussing flights would include a lexicon and grammar rules.</p>
            <div class="grid md:grid-cols-2 gap-4">
                <div>
                    <h5>Lexicon Example[cite: 11]:</h5>
                    <ul class="list-none p-0">
                        <li><strong>Noun:</strong> flights, breeze, trip, morning</li>
                        <li><strong>Verb:</strong> is, prefer, like, need, want, fly</li>
                        <li><strong>Adjective:</strong> cheapest, non-stop, first, latest, other, direct</li>
                        <li><strong>Pronoun:</strong> me, I, you, it</li>
                        <li><strong>Proper-Noun:</strong> Alaska, Baltimore, Los Angeles, Chicago, United, American</li>
                        <li><strong>Determiner:</strong> the, a, an, this, these, that</li>
                        <li><strong>Preposition:</strong> from, to, on, near</li>
                        <li><strong>Conjunction:</strong> and, or, but</li>
                    </ul>
                </div>
                <div>
                    <h5>Grammar Rules Example[cite: 13]:</h5>
                    <ul class="list-none p-0">
                        <li>$S \rightarrow NP \ VP$ (e.g., I + want a morning flight)</li>
                        <li>$NP \rightarrow Pronoun$ (e.g., I)</li>
                        <li>$NP \rightarrow ProperNoun$ (e.g., Los Angeles)</li>
                        <li>$NP \rightarrow Det \ Nominal$ (e.g., a + flight)</li>
                        <li>$Nominal \rightarrow Noun$ (e.g., flights)</li>
                        <li>$Nominal \rightarrow Nominal \ Noun$ (e.g., morning flight)</li>
                        <li>$VP \rightarrow Verb$ (e.g., do)</li>
                        <li>$VP \rightarrow Verb \ NP$ (e.g., want + a flight)</li>
                        <li>$VP \rightarrow Verb \ NP \ PP$ (e.g., leave + Boston + in the morning)</li>
                        <li>$VP \rightarrow Verb \ PP$ (e.g., leaving + on Thursday)</li>
                        <li>$PP \rightarrow Preposition \ NP$ (e.g., from + Los Angeles)</li>
                    </ul>
                </div>
            </div>
            <p>Sentences derivable from a CFG are considered <strong>grammatical</strong>; those that cannot be derived are <strong>ungrammatical</strong>[cite: 15]. A CFG is a <strong>generative grammar</strong> because it defines a language by the set of possible sentences it can generate[cite: 15]. The task of mapping sentences to their parse trees is called <strong>syntactic parsing</strong>[cite: 15].</p>
        </section>

        <section id="grammar-rules-english">
            <h2>Grammar Rules for English</h2>
            <p>Sentences can comprise one or more clauses, where a clause typically represents a 'complete thought'[cite: 16]. Clauses consist of components like Subject, Verb, Object, Complement, and Adverbial[cite: 16]. Understanding clauses is vital for applications such as relation extraction[cite: 16].</p>

            <h3>Noun Phrase (NP) [cite: 17]</h3>
            <p>A noun phrase (e.g., pronoun, proper noun, determiner + nominal) consists of a head noun and various modifiers[cite: 17].</p>
            <ul>
                <li>$NP \rightarrow Det \ Nominal$ (e.g., "The flight was cancelled") [cite: 17]
                    <ul>
                        <li>Determiners ($Det$) can be simple words ('a', 'the') or complex expressions like possessives ($Det \rightarrow NP\ 's$, e.g., "London's mayor's flight")[cite: 17].</li>
                    </ul>
                </li>
                <li>The nominal includes a head noun and optional modifiers (before or after the head)[cite: 17].
                    <div class="formula-box">
                    <code>
                    Nominal $\rightarrow$ Noun <br>
                    Nominal $\rightarrow$ NUM Nominal <br>
                    Nominal $\rightarrow$ Nominal PP <br>
                    Nominal $\rightarrow$ (who|what) VP
                    </code>
                    </div>
                </li>
            </ul>

            <h3>Verb Phrase (VP) [cite: 18]</h3>
            <p>A verb phrase consists of a verb plus other constituents[cite: 18].</p>
            <div class="formula-box">
                <code>
                VP $\rightarrow$ Verb <br>
                VP $\rightarrow$ Verb NP <br>
                VP $\rightarrow$ Verb NP PP <br>
                VP $\rightarrow$ Verb PP (e.g., "leaving on Thursday")
                </code>
            </div>
            <p>Sequential complements involve a VP followed by an embedded sentence: $VP \rightarrow Verb \ S$ (e.g., "You said [you had a lot of money]")[cite: 18].</p>
            <p>Verbs are subcategorized based on the complements they take (their subcategorization frame)[cite: 21, 20, 19]:</p>
            <ul>
                <li><strong>Transitive:</strong> Takes an object (e.g., "they hit the bar")[cite: 19].</li>
                <li><strong>Intransitive:</strong> No object (e.g., "they just ran")[cite: 19].</li>
                <li><strong>Ditransitive:</strong> Direct and indirect object (e.g., "she told me<sup>1</sup> the story<sup>2</sup>")[cite: 19].</li>
                <li><strong>Linking verbs:</strong> Link subject with complement (e.g., "could be right")[cite: 20].</li>
            </ul>
            <p>Modern grammars might have up to 100 subcategories[cite: 21]. A verb can be seen as a predicate: Verb(Arg, Arg, ...) e.g., FIND(I, a flight)[cite: 22, 21].</p>

            <h3>Coordination [cite: 23]</h3>
            <p>Conjunctions (and, or, but) are used for coordination.</p>
            <ul>
                <li>Coordinate NPs: $VP \rightarrow NP \text{ and } NP$ (e.g., "Please repeat the flights<sup>1</sup> and the costs<sup>2</sup>")[cite: 23].</li>
                <li>Coordinate Nominals: $Nominal \rightarrow Nominal \text{ and } Nominal$ (e.g., "Please repeat the flights<sup>1</sup> and costs<sup>2</sup>")[cite: 23].</li>
                <li>Coordinate VPs and Ss: $S \rightarrow S \text{ and } S$; $VP \rightarrow VP \text{ and } VP$ (e.g., "What flights do you have leaving London and arriving in USA?")[cite: 23].</li>
            </ul>
        </section>

        <section id="treebanks-headfinding">
            <h2>Treebanks and Head Finding</h2>
            <p>A <strong>treebank</strong> is a syntactically annotated corpus[cite: 25]. Treebanks often use different tagsets based on the linguistic annotation choices of their authoring projects[cite: 25].</p>
            <p>An example is the <strong>Penn Treebank 3</strong>[cite: 25], which includes newswire and transcribed speech, annotated with sentences, POS tags, and syntactic parse trees[cite: 25]. Example annotations from treebanks:</p>
            <pre><code>
((S
  (NP-SBJ (DT That) (JJ cold) (,) (JJ empty) (NN sky))
  (VP (VBD was)
      (ADJP-PRD (JJ full)
                (PP (IN of)
                    (NP (NN fire))))
      (CC and)
      (NP (NN light)))))

((S
  (NP-SBJ (DT The) (NN flight))
  (VP (MD should)
      (VP (VB arrive)
          (PP-TMP (IN at)
                  (NP (CD eleven) (NN a.m.))) 
          (NP-TMP (NN tomorrow))))))
            </code></pre>
            <p class="text-xs italic">(Note: The provided example in the PDF shows slight variations, e.g. "a.m/RB" and "tomorrow/NN". The structure above is based on the general Penn Treebank style shown.)</p>
            <p>Head finding is the process of identifying the main word (head) in a phrase, which is crucial for dependency parsing and other syntactic analyses.</p>
             <div class="note">
                <p><strong>Scalability (Treebanks & CFGs):</strong> Creating large, accurately annotated treebanks is time-consuming and expensive. Parsing with complex CFGs can be computationally intensive (e.g., CYK algorithm is $O(n^3|G|)$ for sentence length $n$ and grammar size $|G|$). Probabilistic CFGs (PCFGs) are often used to handle ambiguity and improve parsing accuracy by assigning probabilities to rules, but learning these probabilities also requires treebanks. For large-scale applications, efficient parsers and sometimes grammar simplification or specialized parsing algorithms are needed.</p>
            </div>
        </section>

        <section id="cfg-example-nltk">
            <h2>Worked Example CFG (NLTK)</h2>
            <p>The lecture notes provide an NLTK example of a simple CFG and parsing[cite: 26, 27].</p>
            <div class="code-description">
                <h5>Python NLTK CFG Example [cite: 26]</h5>
                <pre><code class="language-python">
import nltk

sent = ['I', 'shot', 'some', 'dinosaurs']

dino_grammar = nltk.CFG.fromstring("""
S -> TOKEN TOKEN TOKEN TOKEN
TOKEN -> N | V | DET | PRO
DET -> 'some'
N -> 'dinosaurs'
V -> 'shot'
PRO -> 'I'
""")

parser = nltk.parse.chart.ChartParser(grammar=dino_grammar, trace=None)
chart = parser.chart_parse(sent)

for tree in chart:
    print(str(tree))
                </code></pre>
                <p>The output trace (partially shown in [cite: 26, 27]) demonstrates the bottom-up chart parsing process, culminating in the successful parse: <code>(S (TOKEN (PRO I)) (TOKEN (V shot)) (TOKEN (DET some)) (TOKEN (N dinosaurs)))</code>.</p>
            </div>
            <p>Another worked example for "I love NLP with parse trees" using a CFG is detailed step-by-step from pages 16-22 of the lecture notes[cite: 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46]. This example illustrates a bottom-up parsing strategy to find a valid parse tree by applying grammar rules from the terminals (words) upwards to the start symbol 'S'.</p>
        </section>

        <section id="dependency-grammars">
            <h2>Dependency Grammars</h2>
            <p>While CFGs provide a constituent-based structure, <strong>Typed Dependency structures</strong> use grammatical relations to connect words[cite: 47].</p>
            <ul>
                <li>Labels for edges are grammatical relations (e.g., nsubj, dobj)[cite: 47].</li>
                <li>Nodes are words[cite: 47].</li>
                <li>Dependency grammars allow for more flexible word order compared to CFGs[cite: 5, 47].</li>
                <li>(Head $\rightarrow$ Dependent) structure approximates semantic relationships[cite: 5].</li>
            </ul>
            <div class="example-box my-8">
                <p class="text-center"><strong>Comparison of CFG and Dependency Parse for "I prefer the morning flight through Denver"</strong> [cite: 7, 47]</p>
                <p class="text-center">(See image on page 5 of the PDF for CFG and page 23 for Dependency tree visualization).</p>
                <p>A dependency parse might show:</p>
                <ul>
                    <li>`prefer` (root)
                        <ul>
                            <li>$\xrightarrow{\text{nsubj}}$ `I`</li>
                            <li>$\xrightarrow{\text{dobj}}$ `flight`
                                <ul>
                                    <li>$\xrightarrow{\text{det}}$ `the`</li>
                                    <li>$\xrightarrow{\text{nmod}}$ `morning`</li>
                                    <li>$\xrightarrow{\text{nmod}}$ `Denver`
                                        <ul>
                                            <li>$\xrightarrow{\text{case}}$ `through`</li>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ul>
            </div>

            <h3>Dependency Relations</h3>
            <p>Grammatical relations connect head and dependent words, with the type of relation called its grammatical function[cite: 48]. Universal Dependencies (UD) provides a common framework for these relations[cite: 50].</p>
            <h4>Examples of Relations[cite: 49, 52]:</h4>
            <div class="overflow-x-auto">
                <table>
                    <thead><tr><th>Relation</th><th>Description</th><th>Example (Head, Dependent)</th></tr></thead>
                    <tbody>
                        <tr><td>NSUBJ</td><td>Nominal subject</td><td>(canceled, United) in "United canceled the flight."</td></tr>
                        <tr><td>DOBJ</td><td>Direct object</td><td>(canceled, flight) in "United canceled the flight."</td></tr>
                        <tr><td>IOBJ</td><td>Indirect object</td><td>(booked, her) in "We booked her the first flight."</td></tr>
                        <tr><td>NMOD</td><td>Nominal modifier</td><td>(flight, Miami) in "the flight to Miami."</td></tr>
                        <tr><td>AMOD</td><td>Adjectival modifier</td><td>(flight, morning) in "the morning flight."</td></tr>
                        <tr><td>NUMMOD</td><td>Numeric modifier</td><td>(flights, 1000) in "canceled 1000 flights."</td></tr>
                        <tr><td>APPOS</td><td>Appositional modifier</td><td>(United, unit) in "United, a unit of UAL..."</td></tr>
                        <tr><td>DET</td><td>Determiner</td><td>(flight, The) in "The flight was canceled."</td></tr>
                        <tr><td>CASE</td><td>Prepositions, postpositions</td><td>(Houston, to) in "flights to Houston."</td></tr>
                        <tr><td>CONJ</td><td>Conjunct</td><td>(Denver, Steamboat) in "to Denver and drove to Steamboat."</td></tr>
                        <tr><td>CC</td><td>Coordinating conjunction</td><td>(Typically connects two conjuncts via CONJ relation)</td></tr>
                    </tbody>
                </table>
            </div>

            <h3>Dependency Formalisms</h3>
            <p>A dependency tree is a directed graph $G=(V,A)$, where $V$ are words (vertices/nodes) and $A$ are grammatical function relationships (arcs)[cite: 54].</p>
            <ul>
                <li>The root node has no incoming arcs[cite: 54].</li>
                <li>Each other vertex has exactly one incoming arc (one head)[cite: 54].</li>
                <li>There's a path from the root to every other vertex[cite: 54].</li>
            </ul>
            <h4>Projectivity:</h4>
            <p>An arc (head, dependent) is 'projective' if there is a path from the head to all words located linearly between the head and the dependent in the sentence[cite: 54].</p>
            <div class="example-box">
                <h5>Example: Non-projectivity [cite: 55]</h5>
                <p>"JetBlue canceled our flight this morning which was already late"</p>
                <p>If `nmod(flight, was)` is a dependency, and "this morning" are between "flight" and "was" but not dependents of "flight" on the path to "was", the arc can be non-projective.</p>
            </div>
            <p>Older parsing algorithms often assumed projective trees[cite: 57]. English phrase-structure derived Treebanks usually guarantee projectivity, but hand-annotated graphs for other languages can include non-projective trees[cite: 57].</p>
             <div class="note">
                <p><strong>Scalability (Dependency Parsing):</strong> Dependency parsers can be more efficient than full CFG parsers for some languages, especially those with freer word order. Transition-based dependency parsers (e.g., using shift-reduce algorithms) can operate in linear time under certain conditions. Graph-based parsers (e.g., finding maximum spanning trees) can be more complex but might handle non-projectivity better. The availability of large multilingual dependency treebanks (like Universal Dependencies) has significantly advanced scalable dependency parsing.</p>
            </div>
        </section>

        <section id="dependency-evaluation">
            <h2>Evaluation of Dependency Parsers</h2>
            <p>Metrics for evaluating dependency graph parsers include[cite: 58]:</p>
            <ul>
                <li><strong>Label Accuracy Score (LAS_edge or LS_incorrectly_named):</strong> Percentage of words with the correct incoming edge label (grammatical relation type), ignoring where the edge came from. (The lecture note "LS" seems to refer to Label Accuracy Score for the edge, but usually LS is not a common acronym here. LAS typically includes the head).
                The slide mentions "percentage of words with correct edge label (ignoring where it came from)". This would be a label-only accuracy for dependents.
                </li>
                <li><strong>Unlabelled Attachment Score (UAS):</strong> Percentage of words correctly assigned their head word. (TP = correct assignment of [word $\rightarrow$ head])[cite: 58].</li>
                <li><strong>Labelled Attachment Score (LAS):</strong> Percentage of words correctly assigned their head word AND the correct grammatical relation label for the dependency. (TP = correct assignment of [word $\rightarrow$ head + dep relation])[cite: 58]. This is the most common primary metric.</li>
                <li><strong>Exact Match:</strong> Percentage of sentences where the entire parse tree is perfectly correct. Very conservative, as minor errors cause failure[cite: 58].</li>
            </ul>
        </section>

    </main>

    <footer class="bg-gray-800 text-white py-8 mt-12">
        <div class="container mx-auto px-6 text-center">
            <p>&copy; <span id="currentYear"></span> NLP Fundamentals & Labs. For educational purposes.</p>
            <p class="text-sm text-gray-400">Content derived from "NLP Exam Preparation" notes and "NLP Labs Detailed Notes".</p>
        </div>
    </footer>

    <script>
        // KaTeX auto-render
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ],
                throwOnError : false
            });

            // Set current year in footer
            document.getElementById('currentYear').textContent = new Date().getFullYear();

            // Active Nav Link Highlighting & Mobile Menu Toggle
            const currentLocation = window.location.pathname.split('/').pop() || 'index.html';
            const navLinks = document.querySelectorAll('nav a.nav-link');
            navLinks.forEach(link => {
                if (link.getAttribute('href') === currentLocation) {
                    link.classList.add('active');
                    link.classList.remove('text-gray-700');
                } else {
                    link.classList.remove('active');
                    link.classList.add('text-gray-700');
                }
            });

            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileMenu = document.getElementById('mobile-menu');
            if (mobileMenuButton && mobileMenu) {
                mobileMenuButton.addEventListener('click', function() {
                    mobileMenu.classList.toggle('hidden');
                });
            }
        });
    </script>

</body>
</html>
