<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Models & Pre-training - Neural NLP Fundamentals</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUbGuHTCQ" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="icon" type="image/png" href="logo.png">
    <style>
        body { font-family: 'Inter', sans-serif; }
        .katex-display { display: block; margin: 1em 0; text-align: center; }
        .content-section h1 { margin-bottom: 1.5rem; font-size: 2.25rem; font-weight: 700; color: #0891b2; /* Cyan-600 */ }
        .content-section h2 { margin-top: 2rem; margin-bottom: 1rem; font-size: 1.75rem; font-weight: 600; border-bottom: 2px solid #e5e7eb; padding-bottom: 0.5rem; }
        .content-section h3 { margin-top: 1.5rem; margin-bottom: 0.75rem; font-size: 1.35rem; font-weight: 600; }
        .content-section h4 { margin-top: 1.25rem; margin-bottom: 0.5rem; font-size: 1.15rem; font-weight: 600; }
        .content-section p, .content-section ul, .content-section ol { margin-bottom: 1rem; line-height: 1.65; color: #374151; /* Gray-700 */ }
        .content-section ul { list-style-type: disc; margin-left: 1.5rem; }
        .content-section ol { list-style-type: decimal; margin-left: 1.5rem; }
        .content-section code { background-color: #f3f4f6; padding: 0.2em 0.4em; margin: 0; font-size: 85%; border-radius: 3px; color: #4b5563; /* Gray-600 */ }
        .content-section pre { background-color: #f9fafb; /* Gray-50 */ border: 1px solid #e5e7eb; /* Gray-200 */ padding: 1em; border-radius: 0.375rem; /* rounded-md */ overflow-x: auto; margin-bottom: 1rem; }
        .content-section pre code { background-color: transparent; padding: 0; font-size: 90%; }
        .content-section table { width: 100%; margin-bottom: 1rem; border-collapse: collapse; box-shadow: 0 1px 3px 0 rgba(0,0,0,.1), 0 1px 2px 0 rgba(0,0,0,.06); border-radius: 0.5rem; overflow: hidden;}
        .content-section th, .content-section td { border: 1px solid #e5e7eb; padding: 0.75rem 1rem; text-align: left; }
        .content-section th { background-color: #f9fafb; /* Gray-50 */ font-weight: 600; color: #1f2937; /* Gray-800 */ }
        .content-section tr:nth-child(even) { background-color: #f9fafb; }
        .nav-link { padding: 0.5rem 1rem; border-radius: 0.375rem; transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out; }
        .nav-link:hover { background-color: #cffafe; /* Cyan-100 */ color: #0e7490; /* Cyan-700 */ }
        .nav-link.active { background-color: #0891b2; /* Cyan-600 */ color: white; }
        .note { background-color: #eef2ff; /* Indigo-50 */ border-left: 4px solid #6366f1; /* Indigo-500 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.25rem;}
        .note strong { color: #4f46e5; /* Indigo-600 */ }
        .example-box { background-color: #ecfeff; /* Cyan-50 */ border: 1px solid #a5f3fc; /* Cyan-200 */ border-left-width: 4px; border-left-color: #22d3ee; /* Cyan-400 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.375rem; }
        .example-box h5 { font-weight: 600; color: #0e7490; /* Cyan-700 */ margin-bottom: 0.5rem; }
        .formula-box { background-color: #f3f4f6; padding: 1rem; border-radius: 0.375rem; margin-bottom:1rem; }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">

    <nav class="bg-white shadow-md sticky top-0 z-50">
        <div class="container mx-auto px-6 py-3 flex flex-wrap justify-between items-center">
            <a href="index.html" class="flex items-center text-xl font-bold text-blue-600">
                <img src="logo.png" alt="NLP Fundamentals Logo" class="h-8 w-auto mr-2">
                <span>NLP Fundamentals</span>
            </a>
            <button id="mobile-menu-button" class="md:hidden text-gray-600 hover:text-gray-800 focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>
            <div id="mobile-menu" class="w-full md:w-auto md:flex md:flex-nowrap md:overflow-x-auto space-y-2 md:space-y-0 md:space-x-1 hidden mt-3 md:mt-0">
                <a href="index.html" class="nav-link text-gray-700 block md:inline-block">Home</a>
                <a href="mind_map.html" class="nav-link text-gray-700 block md:inline-block">Mind Map</a>
                <a href="text_processing.html" class="nav-link text-gray-700 block md:inline-block">Text Processing</a>
                <a href="regex.html" class="nav-link text-gray-700 block md:inline-block">Regex</a>
                <a href="language_models.html" class="nav-link text-gray-700 block md:inline-block">Language Models</a>
                <a href="sequence_labelling.html" class="nav-link text-gray-700 block md:inline-block">Sequence Labelling</a>
                <a href="constituency_grammars.html" class="nav-link text-gray-700 block md:inline-block">Constituency Grammars</a>
                <a href="sparse_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Sparse Embeddings</a>
                <a href="word_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Word Embeddings (Neural)</a>
                <a href="recurrent_neural_networks.html" class="nav-link text-gray-700 block md:inline-block">RNNs</a>
                <a href="seq2seq_attention.html" class="nav-link text-gray-700 block md:inline-block">Seq2Seq & Attention</a>
                <a href="transformer_architecture.html" class="nav-link text-gray-700 block md:inline-block">Transformers</a>
                <a href="transformer_models_pretraining.html" class="nav-link text-gray-700 active block md:inline-block">Transformer Models</a>
                <a href="finetuning_advanced_llm.html" class="nav-link text-gray-700 block md:inline-block">Fine-tuning LLMs</a>
                <a href="nlp_tasks_applications.html" class="nav-link text-gray-700 block md:inline-block">NLP Tasks</a>
                <a href="info_extraction.html" class="nav-link text-gray-700 block md:inline-block">Information Extraction</a>
                <a href="evaluation_metrics_nlp.html" class="nav-link text-gray-700 block md:inline-block">Evaluation Metrics</a>
                <a href="lab_regex.html" class="nav-link text-gray-700 active block md:inline-block">Regex Lab</a>
                <a href="lab_crf.html" class="nav-link text-gray-700 block md:inline-block">CRF Lab</a>
                <a href="lab_bert.html" class="nav-link text-gray-700 block md:inline-block">BERT Lab</a>
                <a href="lab_llama.html" class="nav-link text-gray-700 block md:inline-block">Llama Lab</a>
            </div>
        </div>
    </nav>

    <main class="container mx-auto px-6 py-12 content-section">
        <h1>Section 2.5: Key Transformer-Based Models and Pre-training Objectives</h1>
        <p>
            The Transformer architecture has served as the foundation for a multitude of influential
            language models, each with specific pre-training strategies and architectural nuances
            tailored for different capabilities.
        </p>

        <section id="bert">
            <h2>BERT (Bidirectional Encoder Representations from Transformers)</h2>
            <p>
                BERT, introduced by Devlin et al., revolutionized NLP by enabling the pre-training of
                deep bidirectional representations from unlabeled text.
            </p>
            <h4>Architecture:</h4>
            <p>
                BERT utilizes an <strong>encoder-only</strong> stack of Transformer blocks. A key feature is its use of
                bidirectional self-attention in all layers, allowing each token to attend to both its
                left and right context simultaneously.
            </p>
            <h4>Input Representation:</h4>
            <p>BERT processes input sequences that can be either a single sentence or a pair of sentences. The input representation for each token is a sum of three embeddings:</p>
            <ul>
                <li><strong>Token Embeddings:</strong> WordPiece embeddings (vocab ~30,000 subwords).</li>
                <li><strong>Segment Embeddings:</strong> Learned embeddings ($E_A$ or $E_B$) to distinguish sentences in a pair. For single sentences, only $E_A$ is used.</li>
                <li><strong>Position Embeddings:</strong> Learned absolute position embeddings.</li>
            </ul>
            <h4>Special Tokens:</h4>
            <ul>
                <li><code>[CLS]</code> (Classification): Prepended to every input. Its final hidden state is used as the aggregate sequence representation for classification tasks.</li>
                <li><code>[SEP]</code> (Separator): Separates segments (e.g., two sentences, or question & passage).</li>
            </ul>
            <h4>Pre-training Objectives:</h4>
            <p>BERT is pre-trained on large unlabeled text corpora (BooksCorpus, English Wikipedia) using two novel unsupervised tasks:</p>
            <ol>
                <li>
                    <strong>Masked Language Model (MLM):</strong> Allows BERT to learn bidirectional representations.
                    <ul>
                        <li>~15% of input tokens are randomly masked.</li>
                        <li>The model predicts the original vocabulary ID of these masked tokens based on unmasked left/right context.</li>
                        <li>Masking strategy: Of the 15% chosen:
                            <ul>
                                <li>80% replaced with <code>[MASK]</code> token.</li>
                                <li>10% replaced with a random token.</li>
                                <li>10% kept unchanged.</li>
                            </ul>
                        This forces the model to maintain distributional contextual representations for all input tokens.
                        </li>
                    </ul>
                </li>
                <li>
                    <strong>Next Sentence Prediction (NSP):</strong> Trains the model to understand relationships between sentences.
                    <ul>
                        <li>Two sentences (A, B) are sampled.</li>
                        <li>50% of the time, B is the actual next sentence (label: IsNext).</li>
                        <li>50% of the time, B is a random sentence (label: NotNext).</li>
                        <li>The model uses the final hidden state of <code>[CLS]</code> for this binary classification.</li>
                        <li>NSP was designed for tasks like QA and NLI.</li>
                    </ul>
                </li>
            </ol>
            <div class="example-box">
                <h5>Simplified Example: MLM</h5>
                <p>Input: "My dog is <code>[MASK]</code>."</p>
                <p>BERT's goal: Predict that <code>[MASK]</code> should be "hairy" (or "friendly", "small", etc.) based on "My dog is ...".</p>
                <h5>Simplified Example: NSP</h5>
                <p>Sentence A: "The man went to the store."</p>
                <p>Sentence B (IsNext): "He bought a gallon of milk."</p>
                <p>Sentence B (NotNext): "Penguins are flightless birds."</p>
                <p>BERT predicts if B logically follows A.</p>
            </div>
            <h4>Impact:</h4>
            <p>BERT achieved state-of-the-art results on many NLP tasks through fine-tuning, demonstrating the power of deep bidirectional pre-training.</p>
            <div class="note">
                <p><strong>Scalability (BERT):</strong> Pre-training BERT is computationally very expensive, requiring massive datasets and significant GPU/TPU resources. However, fine-tuning on downstream tasks is much cheaper. The model size (BERT-Base, BERT-Large) also impacts resource needs.</p>
            </div>
        </section>

        <section id="spanbert">
            <h2>SpanBERT</h2>
            <p>Developed by Joshi et al., SpanBERT extends BERT with pre-training methods designed to better represent and predict continuous spans of text.</p>
            <h4>Extensions to BERT:</h4>
            <ul>
                <li>
                    <strong>Span Masking:</strong> Instead of masking random individual tokens, SpanBERT masks contiguous random spans of tokens. Span length is sampled (mean ~3.8 words), starting at a complete word.
                </li>
                <li>
                    <strong>Span Boundary Objective (SBO):</strong> A novel auxiliary objective. SpanBERT predicts the entire content of a masked span using only the representations of observed tokens at its boundaries ($x_{s-1}, x_{e+1}$) and relative position embeddings for tokens within the masked span. This encourages storing span-level info in boundary representations.
                </li>
                <li>
                    <strong>No Next Sentence Prediction (NSP):</strong> Uses single-segment training (single contiguous blocks up to 512 tokens) and omits NSP, allowing learning from longer, coherent contexts.
                </li>
            </ul>
            <h4>Benefits:</h4>
            <p>Substantial performance gains on span selection tasks like extractive Question Answering (SQuAD) and Coreference Resolution.</p>
            <div class="note">
                <p><strong>Scalability (SpanBERT):</strong> Pre-training is comparable in cost to BERT. The architectural changes are minor, but the pre-training task modifications lead to better performance on specific types of downstream tasks without significantly increasing the scaling challenges beyond those of BERT.</p>
            </div>
        </section>

        <section id="roformer">
            <h2>RoFormer (Rotary Transformer)</h2>
            <p>Proposed by Su et al., RoFormer enhances the Transformer architecture by integrating Rotary Position Embedding (RoPE).</p>
            <h4>Key Feature: Rotary Position Embedding (RoPE)</h4>
            <ul>
                <li><strong>Mechanism:</strong> RoPE encodes absolute token position by applying a rotation matrix to its query (Q) and key (K) vectors in self-attention. Rotation angle depends on absolute position. The dot product $Q_m K_n^T$ naturally incorporates relative position $m-n$.</li>
            </ul>
            <h4>Benefits of RoPE:</h4>
            <ul>
                <li><strong>Sequence Length Flexibility:</strong> Better generalization to sequence lengths not seen during training.</li>
                <li><strong>Decaying Inter-token Dependency:</strong> Interaction strength decays with increasing relative distance (intuitive for language).</li>
                <li><strong>Improved Performance:</strong> Demonstrated improvements on tasks like long text classification, faster convergence, and lower loss in language modeling pre-training.</li>
            </ul>
            <div class="note">
                <p><strong>Scalability (RoFormer):</strong> RoPE itself is computationally efficient. The main scalability concerns are those of the underlying Transformer architecture (e.g., quadratic attention complexity). RoPE's ability to handle longer sequences better can be an advantage for scaling to tasks involving extensive contexts.</p>
            </div>
        </section>

        <section id="llama">
            <h2>Llama / Llama 2</h2>
            <p>Llama and Llama 2, from Meta AI, are families of large language models (LLMs) designed to be open and efficient.</p>
            <h4>Architecture:</h4>
            <p>Both are <strong>decoder-only</strong> Transformer models, inherently autoregressive and suited for text generation.</p>
            <h4>Key Architectural Components (Common to Llama 1 & Llama 2):</h4>
            <ul>
                <li><strong>Pre-normalization with RMSNorm:</strong> For improved training stability and efficiency.</li>
                <li><strong>SwiGLU Activation Function:</strong> In feed-forward networks for better performance.</li>
                <li><strong>Rotary Position Embeddings (RoPE):</strong> For encoding positional information.</li>
            </ul>
            <h4>Llama 2 Specific Improvements over Llama 1:</h4>
            <ul>
                <li><strong>Increased Context Length:</strong> 4096 tokens (Llama 1: 2048).</li>
                <li><strong>Grouped-Query Attention (GQA):</strong> For larger models (34B, 70B) to improve inference scalability by sharing key/value projections among query heads, reducing KV cache size.</li>
                <li><strong>Increased Training Data:</strong> 2 trillion tokens (40% increase over Llama 1).</li>
                <li><strong>Data Curation:</strong> More robust cleaning; updated data mixing. Factual sources up-sampled to increase knowledge and reduce hallucinations.</li>
            </ul>
            <h4>Pre-training Data:</h4>
            <p>Pre-trained on large-scale, publicly available text corpora. Llama 2 used 2 trillion tokens from public online data, with efforts to remove data from sites with high personal info volume.</p>
            <div class="note">
                <p><strong>Scalability (Llama/Llama 2):</strong> These are very large models, and their pre-training requires massive computational resources and datasets. Architectural choices like RMSNorm, SwiGLU, and GQA are specifically aimed at improving training stability, performance, and inference efficiency at scale. The decoder-only architecture is well-suited for scaling generative capabilities.</p>
            </div>
        </section>

        <section id="comparison-table-models">
            <h2>Table 3: Key Pre-trained Transformer Models: Architectures and Objectives</h2>
            <div class="overflow-x-auto">
                <table>
                    <thead>
                        <tr>
                            <th>Feature</th>
                            <th>BERT</th>
                            <th>SpanBERT</th>
                            <th>RoFormer</th>
                            <th>Llama 2</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Base Architecture</strong></td>
                            <td>Encoder-only</td>
                            <td>Encoder-only</td>
                            <td>Encoder or Decoder (flexible)</td>
                            <td>Decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>Key Pre-training Objective(s)</strong></td>
                            <td>Masked Language Model (MLM), Next Sentence Prediction (NSP)</td>
                            <td>Span Masking, Span Boundary Objective (SBO)</td>
                            <td>Typically MLM or Causal LM</td>
                            <td>Causal Language Modeling (predict next token)</td>
                        </tr>
                        <tr>
                            <td><strong>Notable Positional Embedding</strong></td>
                            <td>Learned Absolute</td>
                            <td>Learned Absolute</td>
                            <td>Rotary Position Embedding (RoPE)</td>
                            <td>Rotary Position Embedding (RoPE)</td>
                        </tr>
                        <tr>
                            <td><strong>Notable Normalization</strong></td>
                            <td>LayerNorm (post-sublayer)</td>
                            <td>LayerNorm (post-sublayer)</td>
                            <td>LayerNorm or RMSNorm (depending on base)</td>
                            <td>RMSNorm (pre-sublayer)</td>
                        </tr>
                        <tr>
                            <td><strong>Notable FFN Activation</strong></td>
                            <td>GELU</td>
                            <td>GELU</td>
                            <td>Typically ReLU/GELU (RoPE is main feature)</td>
                            <td>SwiGLU</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <p class="mt-4">
                The progression from BERT's MLM to SpanBERT's span objectives, and architectural refinements in RoFormer and Llama, illustrates continuous efforts to enhance representational power, efficiency, and scalability. While BERT established bidirectional pre-training for NLU, Llama models (decoder-only) excel in generation and few-shot learning due to scale and refined components. NSP, central to BERT, is largely abandoned in many subsequent models.
            </p>
        </section>

    </main>

    <footer class="bg-gray-800 text-white py-8 mt-12">
        <div class="container mx-auto px-6 text-center">
            <p>&copy; <span id="currentYear"></span> NLP Fundamentals. For educational purposes.</p>
            <p class="text-sm text-gray-400">Content derived from "NLP Exam Preparation" notes.</p>
        </div>
    </footer>

    <script>
        // KaTeX auto-render
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ],
                throwOnError : false
            });

            // Set current year in footer
            document.getElementById('currentYear').textContent = new Date().getFullYear();

            // Active Nav Link Highlighting & Mobile Menu Toggle
            const currentLocation = window.location.pathname.split('/').pop() || 'index.html';
            const navLinks = document.querySelectorAll('nav a.nav-link');
            navLinks.forEach(link => {
                if (link.getAttribute('href') === currentLocation) {
                    link.classList.add('active');
                    link.classList.remove('text-gray-700');
                } else {
                    link.classList.remove('active');
                    link.classList.add('text-gray-700');
                }
            });

            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileMenu = document.getElementById('mobile-menu');
            if (mobileMenuButton && mobileMenu) {
                mobileMenuButton.addEventListener('click', function() {
                    mobileMenu.classList.toggle('hidden');
                });
            }
        });
    </script>

</body>
</html>
