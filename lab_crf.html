<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lab: Conditional Random Fields (CRFs) - NLP Fundamentals</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" xintegrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" xintegrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUbGuHTCQ" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" xintegrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; }
        .katex-display { display: block; margin: 1em 0; text-align: center; }
        .content-section h1 { margin-bottom: 1.5rem; font-size: 2.25rem; font-weight: 700; color: #e11d48; /* Rose-600 */ }
        .content-section h2 { margin-top: 2rem; margin-bottom: 1rem; font-size: 1.75rem; font-weight: 600; border-bottom: 2px solid #e5e7eb; padding-bottom: 0.5rem; }
        .content-section h3 { margin-top: 1.5rem; margin-bottom: 0.75rem; font-size: 1.35rem; font-weight: 600; }
        .content-section h4 { margin-top: 1.25rem; margin-bottom: 0.5rem; font-size: 1.15rem; font-weight: 600; }
        .content-section p, .content-section ul, .content-section ol { margin-bottom: 1rem; line-height: 1.65; color: #374151; /* Gray-700 */ }
        .content-section ul { list-style-type: disc; margin-left: 1.5rem; }
        .content-section ol { list-style-type: decimal; margin-left: 1.5rem; }
        .content-section code { background-color: #f3f4f6; padding: 0.2em 0.4em; margin: 0; font-size: 85%; border-radius: 3px; color: #4b5563; /* Gray-600 */ }
        .content-section pre { background-color: #f9fafb; /* Gray-50 */ border: 1px solid #e5e7eb; /* Gray-200 */ padding: 1em; border-radius: 0.375rem; /* rounded-md */ overflow-x: auto; margin-bottom: 1rem; }
        .content-section pre code { background-color: transparent; padding: 0; font-size: 90%; color: #1f2937; }
        .content-section table { width: 100%; margin-bottom: 1rem; border-collapse: collapse; box-shadow: 0 1px 3px 0 rgba(0,0,0,.1), 0 1px 2px 0 rgba(0,0,0,.06); border-radius: 0.5rem; overflow: hidden;}
        .content-section th, .content-section td { border: 1px solid #e5e7eb; padding: 0.75rem 1rem; text-align: left; font-size: 0.875rem; }
        .content-section th { background-color: #f9fafb; /* Gray-50 */ font-weight: 600; color: #1f2937; /* Gray-800 */ }
        .content-section tr:nth-child(even) { background-color: #f9fafb; }
        .nav-link { padding: 0.5rem 1rem; border-radius: 0.375rem; transition: background-color 0.2s ease-in-out, color 0.2s ease-in-out; }
        .nav-link:hover { background-color: #ffe4e6; /* Rose-100 */ color: #be123c; /* Rose-700 */ }
        .nav-link.active { background-color: #e11d48; /* Rose-600 */ color: white; }
        .note { background-color: #eef2ff; /* Indigo-50 */ border-left: 4px solid #6366f1; /* Indigo-500 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.25rem;}
        .note strong { color: #4f46e5; /* Indigo-600 */ }
        .example-box { background-color: #fff1f2; /* Rose-50 */ border: 1px solid #fecdd3; /* Rose-200 */ border-left-width: 4px; border-left-color: #fda4af; /* Rose-300 */ padding: 1rem; margin-top: 1rem; margin-bottom: 1.5rem; border-radius: 0.375rem; }
        .example-box h5 { font-weight: 600; color: #be123c; /* Rose-700 */ margin-bottom: 0.5rem; }
        .formula-box { background-color: #f3f4f6; padding: 1rem; border-radius: 0.375rem; margin-bottom:1rem; text-align:center; }
        .code-description { background-color: #f8f8f8; border: 1px solid #eee; padding: 1em; margin-bottom: 1em; border-radius: 5px; }
        .code-description h5 { font-weight: bold; margin-bottom: 0.5em; color: #555; }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">

    <nav class="bg-white shadow-md sticky top-0 z-50">
        <div class="container mx-auto px-6 py-3 flex flex-wrap justify-between items-center">
            <a href="index.html" class="text-xl font-bold text-blue-600">NLP Fundamentals</a>
            <button id="mobile-menu-button" class="md:hidden text-gray-600 hover:text-gray-800 focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>
            <div id="mobile-menu" class="w-full md:w-auto md:flex md:flex-nowrap md:overflow-x-auto space-y-2 md:space-y-0 md:space-x-1 hidden mt-3 md:mt-0">
                <a href="index.html" class="nav-link text-gray-700 block md:inline-block">Home</a>
                <a href="text_processing.html" class="nav-link text-gray-700 block md:inline-block">Text Processing</a>
                <a href="regex.html" class="nav-link text-gray-700 block md:inline-block">Regex</a>
                <a href="language_models.html" class="nav-link text-gray-700 block md:inline-block">Language Models</a>
                <a href="sequence_labelling.html" class="nav-link text-gray-700 block md:inline-block">Sequence Labelling</a>
                <a href="constituency_grammars.html" class="nav-link text-gray-700 block md:inline-block">Constituency Grammars</a>
                <a href="sparse_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Sparse Embeddings</a>
                <a href="basic_text_processing_foundations.html" class="nav-link text-gray-700 block md:inline-block">Basic Text Processing</a>
                <a href="word_embeddings.html" class="nav-link text-gray-700 block md:inline-block">Word Embeddings (Neural)</a>
                <a href="recurrent_neural_networks.html" class="nav-link text-gray-700 block md:inline-block">RNNs</a>
                <a href="seq2seq_attention.html" class="nav-link text-gray-700 block md:inline-block">Seq2Seq & Attention</a>
                <a href="transformer_architecture.html" class="nav-link text-gray-700 block md:inline-block">Transformers</a>
                <a href="transformer_models_pretraining.html" class="nav-link text-gray-700 block md:inline-block">Transformer Models</a>
                <a href="finetuning_advanced_llm.html" class="nav-link text-gray-700 block md:inline-block">Fine-tuning LLMs</a>
                <a href="nlp_tasks_applications.html" class="nav-link text-gray-700 block md:inline-block">NLP Tasks</a>
                <a href="evaluation_metrics_nlp.html" class="nav-link text-gray-700 block md:inline-block">Evaluation Metrics</a>
                <a href="lab_regex.html" class="nav-link text-gray-700 block md:inline-block">Regex Lab</a>
                <a href="lab_crf.html" class="nav-link text-gray-700 active block md:inline-block">CRF Lab</a>
                <a href="lab_bert.html" class="nav-link text-gray-700 block md:inline-block">BERT Lab</a>
                <a href="lab_llama.html" class="nav-link text-gray-700 block md:inline-block">Llama Lab</a>
            </div>
        </div>
    </nav>

    <main class="container mx-auto px-6 py-12 content-section">
        <h1>Lab 2: Conditional Random Fields (CRFs) for Sequence Labeling</h1>
        <p>
            Conditional Random Fields (CRFs) are a class of statistical modeling methods often applied
            to structured prediction tasks, most notably sequence labeling in NLP. They offer a
            probabilistic framework for segmenting and labeling sequential data.
        </p>

        <section id="lab-crf-experiments">
            <h2>2.1. Analysis of CRF Lab Experiments</h2>
            <p>
                The lab experiments explore NER using CRFs, focusing on the impact of different feature engineering strategies and CRF training model configurations. The dataset consists of 12751 training sentences and 76 test sentences, with a maximum of 20 training iterations for most tasks. The <code>exec_task(...)</code> function orchestrates each experiment.
            </p>

            <h4>Core Lab Components:</h4>
            <div class="code-description">
                <h5><code>create_dataset(max_files=None)</code>:</h5>
                <p>Loads parsed OntoNotes data, splits into 90% train / 10% test. Processes sentences into <code>(token, POS_tag, NER_IOB_tag)</code> tuples. Filters sentences with 'XX' or 'VERB' POS tags (potential parsing issues/dataset characteristics).</p>
            </div>
            <div class="code-description">
                <h5>Feature Extraction Functions (<code>word2features</code>):</h5>
                <p>Convert words into feature dictionaries for the CRF.</p>
                <ul>
                    <li><strong><code>task1_word2features(sent, i)</code>:</strong> Basic features: current word/POS, previous/next word (lowercase)/POS, BOS/EOS flags.</li>
                    <li><strong><code>task2_word2features(sent, i)</code>:</strong> Enhanced features: includes all from task1, plus for current, previous, and next words: word shape (lower, isupper, istitle, isdigit), word suffix (last 3 chars), and POS tag prefix (first 2 chars).</li>
                </ul>
            </div>
             <div class="code-description">
                <h5>Sentence Conversion Utilities:</h5>
                <ul>
                    <li><code>sent2features(sent, word2features_func)</code>: Applies chosen <code>word2features</code> to each word.</li>
                    <li><code>sent2labels(sent)</code>: Extracts IOB labels.</li>
                    <li><code>sent2tokens(sent)</code>: Extracts tokens.</li>
                </ul>
            </div>
            <div class="code-description">
                <h5>CRF Model Training Functions:</h5>
                <p>The core of the training process in Tasks 1-4 involves instantiating and fitting a <code>sklearn_crfsuite.CRF</code> model. Task 5 uses a similar CRF model but wraps it in a hyperparameter search.</p>
                <ul>
                    <li>
                        <strong><code>task1_train_crf_model(X_train, Y_train, max_iter, labels)</code> (Basis for Tasks 1, 2, 3, 4):</strong>
                        <p>This function trains the CRF model using <code>sklearn_crfsuite.CRF</code>. Let's break down its components:</p>
                        <pre><code class="language-python">
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs',
    c1=0.1,  # L1 penalty
    c2=0.1,  # L2 penalty
    max_iterations=max_iter,
    all_possible_transitions=False # For Task 1 & 2. Task 4 sets this to True.
)
crf.fit(X_train, Y_train)
                        </code></pre>
                        <ul>
                            <li><code>sklearn_crfsuite.CRF(...)</code>: This initializes the Conditional Random Field model. The goal of training is to find the optimal set of feature weights ($\lambda_k$ in the CRF formula) that maximize the conditional log-likelihood of the training data.</li>
                            <li><code>algorithm='lbfgs'</code>: This specifies the optimization algorithm used to find the best feature weights. <strong>L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)</strong> is a popular quasi-Newton method.
                                <ul>
                                    <li>It approximates the inverse Hessian matrix (second derivatives) to steer the search for the optimal weights, making it faster than gradient descent alone for many problems.</li>
                                    <li>"Limited-memory" means it doesn't store the full dense approximation of the inverse Hessian, but rather a few vectors that implicitly represent it. This makes it suitable for problems with a large number of variables (features), which is common in NLP.</li>
                                    <li>The L-BFGS algorithm iteratively updates the weights to minimize the negative log-likelihood (which is equivalent to maximizing the log-likelihood) of the CRF model given the training data and features.</li>
                                </ul>
                            </li>
                            <li><code>c1=0.1</code>: This is the coefficient for L1 regularization. L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. This can lead to sparse solutions where some feature weights become zero, effectively performing feature selection.</li>
                            <li><code>c2=0.1</code>: This is the coefficient for L2 regularization. L2 regularization adds a penalty equal to the square of the magnitude of coefficients. This tends to shrink weights towards zero but rarely makes them exactly zero, preventing overfitting by discouraging overly complex models where individual features have too much influence.</li>
                            <li><code>max_iterations=max_iter</code>: The maximum number of iterations the L-BFGS algorithm will run.</li>
                            <li><code>all_possible_transitions=False</code>: If <code>False</code> (Tasks 1, 2, 3), the model only considers transitions between labels that were observed in the training data. If <code>True</code> (Task 4, and base CRF in Task 5), it learns weights for all possible transitions between labels, even if they weren't seen.</li>
                            <li><code>crf.fit(X_train, Y_train)</code>: This method executes the L-BFGS optimization algorithm to learn the feature weights from the training feature sequences (<code>X_train</code>) and their corresponding label sequences (<code>Y_train</code>).</li>
                        </ul>
                        <p>Tasks 2, 3, and 4 use the same fundamental training logic as Task 1 but may vary parameters like <code>c1</code> (Task 3) or <code>all_possible_transitions</code> (Task 4) passed to the <code>sklearn_crfsuite.CRF</code> constructor.</p>
                    </li>
                    <li>
                        <strong><code>task3_train_crf_model(...)</code>:</strong>
                        <p>Identical to <code>task1_train_crf_model</code> but sets <code>c1=200</code>, significantly increasing the L1 regularization penalty to encourage a much sparser model.</p>
                    </li>
                    <li>
                        <strong><code>task4_train_crf_model(...)</code>:</strong>
                        <p>Identical to <code>task1_train_crf_model</code> but sets <code>all_possible_transitions=True</code>, allowing the model to learn weights for transitions between labels not explicitly observed in the training set.</p>
                    </li>
                    <li>
                        <strong><code>task5_train_crf_model(X_train, Y_train, max_iter, labels)</code>:</strong>
                        <p>This function implements hyperparameter optimization using <code>sklearn.model_selection.RandomizedSearchCV</code> to find the best <code>c1</code> and <code>c2</code> values.</p>
                        <pre><code class="language-python">
# Base CRF model for the search
crf = sklearn_crfsuite.CRF(
    algorithm='lbfgs', 
    max_iterations=max_iter, 
    all_possible_transitions=True # Note: Base model here uses True
)

# Define the hyperparameter space to search
params_space = {
    'c1': scipy.stats.expon(scale=0.5),  # Sample c1 from an exponential distribution
    'c2': scipy.stats.expon(scale=0.05), # Sample c2 from an exponential distribution
}

# Define the scoring metric
f1_scorer = make_scorer(sklearn_crfsuite.metrics.flat_f1_score, 
                        average='weighted', labels=labels)

# Setup RandomizedSearchCV
rs = sklearn.model_selection.RandomizedSearchCV(
    crf, params_space, 
    cv=3,  # 3-fold cross-validation
    verbose=1, 
    n_jobs=-1, # Use all available CPU cores
    n_iter=50, # Number of parameter settings that are sampled
    scoring=f1_scorer
)
rs.fit(X_train, Y_train) # Perform the search

# ... (rest of the code for printing results and plotting) ...

crf = rs.best_estimator_ # Get the model with the best found parameters
                        </code></pre>
                        <ul>
                            <li><code>sklearn_crfsuite.CRF(...)</code>: A base CRF model is initialized, again using L-BFGS. Note that <code>all_possible_transitions</code> is set to <code>True</code> for the base model in this search.</li>
                            <li><code>params_space</code>: This dictionary defines the search space for hyperparameters. <code>c1</code> and <code>c2</code> values are sampled from exponential distributions (<code>scipy.stats.expon</code>). An exponential distribution is often used for regularization parameters because it tends to sample smaller values more frequently but can still explore larger values. The <code>scale</code> parameter influences the mean of the distribution.</li>
                            <li><code>f1_scorer = make_scorer(...)</code>: Since <code>RandomizedSearchCV</code> needs a scoring function to evaluate parameter combinations, <code>make_scorer</code> is used to create one. It utilizes <code>sklearn_crfsuite.metrics.flat_f1_score</code>, which calculates the F1 score entity by entity and then averages them. The <code>average='weighted'</code> parameter means the F1 score for each label is weighted by its support (number of true instances for that label) when calculating the overall average. This is the metric the search aims to maximize.</li>
                            <li><code>RandomizedSearchCV(...)</code>:
                                <ul>
                                    <li><code>crf</code>: The estimator (CRF model) to tune.</li>
                                    <li><code>params_space</code>: The distributions from which to sample parameters.</li>
                                    <li><code>cv=3</code>: Specifies 3-fold cross-validation. The training data is split into 3 parts; the model trains on 2 parts and validates on the 3rd, rotating through the parts. This helps get a more robust estimate of performance for each parameter combination.</li>
                                    <li><code>verbose=1</code>: Controls the verbosity of messages during the search.</li>
                                    <li><code>n_jobs=-1</code>: Uses all available CPU cores to parallelize the search if possible (each parameter combination and CV fold can often be trained independently).</li>
                                    <li><code>n_iter=50</code>: The number of different parameter combinations that will be sampled and evaluated. Unlike GridSearchCV which tries all combinations, RandomizedSearchCV samples a fixed number.</li>
                                    <li><code>scoring=f1_scorer</code>: The custom F1 scorer defined above.</li>
                                </ul>
                            </li>
                            <li><code>rs.fit(X_train, Y_train)</code>: Starts the randomized search process. It will sample 50 combinations of <code>c1</code> and <code>c2</code>, train a CRF model for each using 3-fold cross-validation, and evaluate it using the weighted F1 score.</li>
                            <li>The subsequent lines in the original <code>task5_train_crf_model</code> function print the best parameters found (<code>rs.best_params_</code>), the corresponding best F1 score (<code>rs.best_score_</code>), and the size of the best model. It also includes optional code to visualize the F1 scores across the sampled <code>c1</code> and <code>c2</code> values using <code>matplotlib</code>, helping to understand the hyperparameter landscape.</li>
                            <li><code>crf = rs.best_estimator_</code>: Finally, the function returns the CRF model that was trained with the hyperparameter combination that yielded the highest weighted F1 score during the cross-validated search.</li>
                        </ul>
                    </li>
                </ul>
            </div>

            <div class="code-description mt-4">
                <h4>Detailed Look at CRF Training</h4>
                <p>The core of the CRF training in this lab revolves around the <code>sklearn_crfsuite.CRF</code> estimator, which uses the L-BFGS algorithm for optimization.</p>

                <h5>The <code>sklearn_crfsuite.CRF</code> Estimator</h5>
                <p><code>sklearn_crfsuite.CRF</code> provides an interface similar to scikit-learn estimators for training and using Conditional Random Fields. Key aspects include:</p>
                <ul>
                    <li><strong>Feature Input:</strong> It expects features for each token in a sequence, typically as a list of dictionaries per sequence.</li>
                    <li><strong>Optimization:</strong> It internally handles the complex process of finding the optimal weights for the features and transitions that maximize the likelihood of the training data.</li>
                    <li>For more details, refer to the <a href="https://sklearn-crfsuite.readthedocs.io/en/latest/index.html" target="_blank" class="text-rose-600 hover:text-rose-800 underline">sklearn-crfsuite documentation</a>.</li>
                </ul>

                <h5>The L-BFGS Optimization Algorithm</h5>
                <p>L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) is a popular quasi-Newton optimization algorithm. It's well-suited for problems with a large number of variables, like training CRFs where each feature and potential label transition has a weight to be learned.</p>
                <ul>
                    <li><strong>How it works:</strong> L-BFGS approximates the inverse Hessian matrix (which describes the curvature of the loss function) to find the search direction for updating weights. Unlike standard BFGS, it doesn't store the full dense Hessian approximation, but rather a limited history of updates to vectors, making it memory-efficient.</li>
                    <li><strong>In CRFs:</strong> It iteratively adjusts the model's feature weights to minimize a loss function (typically the negative log-likelihood of the training data, often with regularization terms).</li>
                </ul>

                <h5>Standard Training (Tasks 1-4 using <code>task1_train_crf_model</code> as an example)</h5>
                <p>For tasks 1 through 4 (with variations in features or specific parameters like <code>c1</code> or <code>all_possible_transitions</code>), the training approach is direct. Let's look at the <code>task1_train_crf_model</code> code:</p>
                <pre><code class="language-python">
def task1_train_crf_model( X_train, Y_train, max_iter, labels ) :
    # train the basic CRF model
    crf = sklearn_crfsuite.CRF(
        algorithm='lbfgs',
        c1=0.1,
        c2=0.1,
        max_iterations=max_iter,
        all_possible_transitions=False,
    )
    crf.fit(X_train, Y_train)
    return crf
                </code></pre>
                <p>Key parameters here:</p>
                <ul>
                    <li><code>algorithm='lbfgs'</code>: Explicitly selects the L-BFGS optimizer.</li>
                    <li><code>c1=0.1</code>: Coefficient for L1 regularization. L1 regularization encourages sparsity by pushing some feature weights to zero, effectively performing feature selection. A value of 0.1 provides moderate L1 regularization.</li>
                    <li><code>c2=0.1</code>: Coefficient for L2 regularization. L2 regularization penalizes large weights, helping to prevent overfitting by encouraging smaller, more distributed weights. A value of 0.1 provides moderate L2 regularization.</li>
                    <li><code>max_iterations=max_iter</code>: The maximum number of iterations the L-BFGS algorithm will run. Training stops if convergence is reached earlier or this limit is hit.</li>
                    <li><code>all_possible_transitions=False</code>: When set to <code>False</code>, the model only considers transitions between labels that were observed in the training data. If <code>True</code>, it would learn weights for all possible transitions between any two labels, even if never seen.</li>
                </ul>
                <p>The <code>crf.fit(X_train, Y_train)</code> call then executes the L-BFGS algorithm to learn the weights based on the provided training features (<code>X_train</code>) and corresponding labels (<code>Y_train</code>).</p>

                <h5>Hyperparameter Tuning with <code>RandomizedSearchCV</code> (Task 5 using <code>task5_train_crf_model</code>)</h5>
                <p>Task 5 employs a more sophisticated approach to find optimal regularization parameters (<code>c1</code> and <code>c2</code>) using randomized search with cross-validation.</p>
                <pre><code class="language-python">
def task5_train_crf_model( X_train, Y_train, max_iter, labels ) :
    # randomized search to discover best parameters for CRF model
    crf = sklearn_crfsuite.CRF(
        algorithm='lbfgs', 
        max_iterations=max_iter, 
        all_possible_transitions=True  # Note: Base CRF here uses True
    )
    params_space = {
        'c1': scipy.stats.expon(scale=0.5),
        'c2': scipy.stats.expon(scale=0.05),
    }

    # optimize for micro F1 score
    f1_scorer = make_scorer( sklearn_crfsuite.metrics.flat_f1_score, average='weighted', labels=labels )

    print( 'starting randomized search for hyperparameters' )
    n_folds = 3
    n_candidates = 50
    rs = sklearn.model_selection.RandomizedSearchCV(
        crf, params_space, 
        cv=n_folds, verbose=1, n_jobs=-1, 
        n_iter=n_candidates, scoring=f1_scorer
    )
    rs.fit(X_train, Y_train)

    # ... (output and visualization code omitted for brevity) ...

    # return the best model
    crf = rs.best_estimator_
    return crf
                </code></pre>
                <p>Key components of this approach:</p>
                <ul>
                    <li><strong>Base CRF Model:</strong> A <code>sklearn_crfsuite.CRF</code> instance is created, notably with <code>all_possible_transitions=True</code>. This means the hyperparameter search is performed on a model that considers all potential label transitions.</li>
                    <li><strong><code>params_space</code>:</strong> Defines the search space for hyperparameters.
                        <ul></ul>
                            <li><code>'c1': scipy.stats.expon(scale=0.5)</code>: Values for <code>c1</code> (L1 regularization) are sampled from an exponential distribution with a scale parameter of 0.5. This distribution tends to produce smaller values more frequently but can also sample larger values.</li>
                            <li><code>'c2': scipy.stats.expon(scale=0.05)</code>: Values for <code>c2</code> (L2 regularization) are sampled from an exponential distribution with a smaller scale of 0.05, suggesting a search biased towards smaller L2 penalties.</li>
                        </ul>
                    </li>
                    <li><strong><code>f1_scorer</code>:</strong> Specifies the metric to optimize. <code>make_scorer(sklearn_crfsuite.metrics.flat_f1_score, average='weighted', labels=labels)</code> creates a scorer that calculates the weighted flat F1-score. "Flat" means it considers all labels in a flattened sequence, and "weighted" means the F1-score for each label is averaged, weighted by the number of true instances for each label. This is a common metric for sequence labeling tasks.</li>
                    <li><strong><code>RandomizedSearchCV</code>:</strong>
                        <ul>
                            <li><code>cv=n_folds</code> (e.g., 3): Performs 3-fold cross-validation. The training data is split into 3 parts; the model trains on 2 parts and validates on the 3rd, rotating which part is used for validation. This helps get a more robust estimate of performance for each hyperparameter combination.</li>
                            <li><code>n_iter=n_candidates</code> (e.g., 50): Specifies the number of different hyperparameter combinations to try. For each iteration, a random combination of <code>c1</code> and <code>c2</code> is drawn from <code>params_space</code>.</li>
                            <li><code>scoring=f1_scorer</code>: Uses the defined F1 scorer to evaluate each combination.</li>
                            <li><code>n_jobs=-1</code>: Uses all available CPU cores for parallel processing, speeding up the search.</li>
                        </ul>
                    </li>
                    <li><strong><code>rs.fit(X_train, Y_train)</code>:</strong> Executes the randomized search. It trains and evaluates <code>n_candidates</code> models, each with different <code>c1</code> and <code>c2</code> values, using <code>n_folds</code> cross-validation.</li>
                    <li><strong><code>rs.best_estimator_</code>:</strong> After the search, this attribute holds the CRF model that was trained on the full training data (<code>X_train</code>, <code>Y_train</code>) using the hyperparameter combination that yielded the best average F1 score during cross-validation.</li>
                </ul>
                <p>The visualization code (using <code>matplotlib</code>) then helps to understand how different combinations of <code>c1</code> and <code>c2</code> affected the F1 score, providing insights into the sensitivity of the model to these regularization parameters.</p>
            </div>


            <h3 id="lab-crf-task1" class="mt-6">Task 1: Baseline Features, Basic Training</h3>
            <p><strong>Feature Function:</strong> <code>task1_word2features</code> (basic word/POS context, BOS/EOS).</p>
            <p><strong>Training Function:</strong> <code>task1_train_crf_model</code> (<code>c1=0.1, c2=0.1, all_possible_transitions=False</code>).</p>
            <p><strong>Example Training Feature (from logs):</strong> <code>{'word': 'have', 'postag': 'VBP', '-1:word.lower()': 'people', '-1:postag': 'NNS', '+1:word.lower()': 'long', '+1:postag': 'RB'}</code>.</p>
            <p><strong>Performance Highlights (F1 from logs):</strong> B-GPE (0.69), I-GPE (0.00), B-PERSON (0.51), I-PERSON (0.68), B-ORG (0.40), I-ORG (0.53). Overall Micro F1: 0.54.</p>
            <p><strong>Top Observation Features (Examples from logs):</strong></p>
            <ul>
                <li>For $y=\text{'B-DATE'}$, <code>word:today</code> (+2.506), <code>postag:NN</code> (+1.519).</li>
                <li>For $y=\text{'B-GPE'}$, <code>word:US</code> (+2.947), <code>postag:NNP</code> (+2.155).</li>
                <li>For $y=\text{'B-PERSON'}$, <code>postag:NNP</code> (+3.344), <code>-1:word.lower():president</code> (+1.955).</li>
                <li>For $y=\text{'O'}$, <code>EOS</code> (+6.676), <code>BOS</code> (+3.138), <code>postag:NNP</code> (-4.241, strong negative).</li>
            </ul>
            <p><strong>Label Transition Weights (Examples from logs):</strong> Strong positive for B-DATE $\rightarrow$ I-DATE (+6.261), O $\rightarrow$ O (+5.649). O $\rightarrow$ B-PERSON (+3.415) is a likely start.</p>
            <p><strong>Interpretation:</strong> Moderate baseline performance. Relies heavily on direct lexical cues (specific words like "US", "today") and POS tags (NNP for entities). The model struggles with continuation tags like I-GPE (F1=0.00) and many other less frequent entity types, indicating the feature set is too simple for nuanced distinctions or these entities are rare. The negative weight for NNP being 'O' is logical. High B-GPE recall (0.83) vs. precision (0.59) suggests it identifies many geographical entities but also over-generates them.</p>

            <h3 id="lab-crf-task2" class="mt-6">Task 2: Enhanced Features, Basic Training</h3>
            <p><strong>Feature Function:</strong> <code>task2_word2features</code> (adds word shape, suffix, POS prefix for current/prev/next words).</p>
            <p><strong>Training Function:</strong> <code>task1_train_crf_model</code> (same as Task 1).</p>
            <p><strong>Example Training Feature (from logs):</strong> <code>{'word': 'have', 'postag': 'VBP', 'word.lower()': 'have', 'word.isupper()': False, ..., 'word.suffix': 'ave', 'postag[:2]': 'VB', ...}</code>.</p>
            <p><strong>Performance Highlights (F1 from logs):</strong> B-GPE (0.69), I-GPE (0.00), B-PERSON (0.69, up), I-PERSON (0.81, up), B-ORG (0.46, up), I-ORG (0.64, up). Overall Micro F1: 0.66 (up from 0.54).</p>
            <p><strong>Top Observation Features (Examples from logs):</strong> Features like <code>word.suffix:day</code> (+2.210 for B-DATE), <code>word.lower():us</code> (+1.688 for B-GPE), and <code>word.istitle()</code> (+1.201 for B-PERSON) now appear with high weights.</p>
            <p><strong>Interpretation:</strong> The richer orthographic and morphological features significantly boost NER performance, especially for PERSON and ORG entities. Features like <code>word.istitle()</code> are strong indicators for proper nouns. This task clearly shows the impact of feature engineering: better features lead to better model performance with the same training algorithm.</p>

            <h3 id="lab-crf-task3" class="mt-6">Task 3: Enhanced Features, Strong L1 Regularization</h3>
            <p><strong>Feature Function:</strong> <code>task2_word2features</code> (same rich features as Task 2).</p>
            <p><strong>Training Function:</strong> <code>task3_train_crf_model</code> (key difference: <code>c1=200</code> for strong L1 penalty).</p>
            <p><strong>Performance Highlights (F1 from logs):</strong> Dramatic decline. B-GPE (0.43), B-PERSON (0.12). Overall Micro F1: 0.18.</p>
            <p><strong>Feature/Transition Weights (from logs):</strong> Magnitudes considerably smaller than Task 2.</p>
            <p><strong>Interpretation:</strong> The strong L1 regularization (<code>c1=200</code>) likely zeroed out too many feature weights, leading to an overly sparse and underfit model. Despite having rich features, the model couldn't learn effectively. This highlights that aggressive regularization can be detrimental if not tuned properly.</p>

            <h3 id="lab-crf-task4" class="mt-6">Task 4: Enhanced Features, All Possible Transitions</h3>
            <p><strong>Feature Function:</strong> <code>task2_word2features</code> (rich features).</p>
            <p><strong>Training Function:</strong> <code>task4_train_crf_model</code> (key difference: <code>all_possible_transitions=True</code>; <code>c1, c2</code> back to <code>0.1</code>).</p>
            <p><strong>Performance Highlights (F1 from logs):</strong> Mixed. Overall Micro F1 (0.59) lower than Task 2. B-GPE (0.69), B-PERSON (0.59, worse than T2), I-PERSON (0.82, slightly better than T2), B-ORG (0.08, much worse).</p>
            <p><strong>Interpretation:</strong> Allowing all possible transitions means the model considers transitions not seen in training. While this could theoretically help generalize, in this case, it seems to have degraded performance for some entity types (especially B-ORG), perhaps by introducing noise or diluting learned patterns from observed transitions. The improvement in I-PERSON might suggest it helped with some specific continuation patterns not well-covered otherwise.</p>

            <h3 id="lab-crf-task5" class="mt-6">Task 5: Enhanced Features, Hyperparameter Optimization</h3>
            <p><strong>Feature Function:</strong> <code>task2_word2features</code> (rich features).</p>
            <p><strong>Training Function:</strong> <code>task5_train_crf_model</code> (<code>RandomizedSearchCV</code> for <code>c1, c2</code>; base CRF uses <code>all_possible_transitions=True</code>).</p>
            <p><strong>Best Hyperparameters Found (from logs):</strong> c1 = 0.154, c2 = 0.025.</p>
            <p><strong>Best Micro F1 during search (from logs):</strong> 0.4907.</p>
            <p><strong>Performance (final model with best params, from logs):</strong> B-GPE F1 (0.61), B-PERSON (0.36). Overall Micro F1: 0.40.</p>
            <p><strong>Interpretation:</strong> Hyperparameter optimization did not yield improvements and resulted in performance worse than Task 2 and Task 1. The best score found during the search (0.49) was already low. This suggests issues like: the underlying trainer in <code>task5_train_crf_model</code> might be less effective, the search space/iterations insufficient, or overfitting to CV splits. The <code>UndefinedMetricWarning</code> messages during HPO also indicate problems. This task underscores that HPO is not a magic bullet and depends heavily on the base model, search strategy, and data.</p>

            <h4 class="mt-8">Comparative Performance Summary & Discussion</h4>
            <div class="overflow-x-auto">
                <table>
                    <thead>
                        <tr>
                            <th>Task</th><th>Feature Set</th><th>Training Key Params</th><th>Micro F1</th><th>B-GPE F1</th><th>B-PERSON F1</th><th>I-PERSON F1</th><th>B-ORG F1</th><th>Comment</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td>Task 1</td><td>Basic</td><td><code>c1=0.1, c2=0.1, APT=F</code></td><td>0.54</td><td>0.69</td><td>0.51</td><td>0.68</td><td>0.40</td><td>Baseline performance.</td></tr>
                        <tr><td>Task 2</td><td>Enhanced</td><td><code>c1=0.1, c2=0.1, APT=F</code></td><td>0.66</td><td>0.69</td><td>0.69</td><td>0.81</td><td>0.46</td><td><strong>Best overall.</strong> Richer features significantly helped.</td></tr>
                        <tr><td>Task 3</td><td>Enhanced</td><td><code>c1=200, APT=F</code></td><td>0.18</td><td>0.43</td><td>0.12</td><td>0.16</td><td>0.00</td><td>Strong L1 penalty likely caused underfitting.</td></tr>
                        <tr><td>Task 4</td><td>Enhanced</td><td><code>c1=0.1, c2=0.1, APT=T</code></td><td>0.59</td><td>0.69</td><td>0.59</td><td>0.82</td><td>0.08</td><td><code>all_possible_transitions=True</code> had mixed results, hurt B-ORG.</td></tr>
                        <tr><td>Task 5</td><td>Enhanced</td><td>HPO (<code>APT=T</code> base)</td><td>0.40</td><td>0.61</td><td>0.36</td><td>0.11</td><td>0.41</td><td>HPO failed to find better parameters, performed poorly.</td></tr>
                    </tbody>
                </table>
                <p class="text-xs mt-1">APT = all_possible_transitions</p>
            </div>
            <p><strong>Discussion on Performance Differences:</strong></p>
            <ul>
                <li><strong>Feature Engineering is Key (Task 1 vs. Task 2):</strong> The most significant jump in performance came from enhancing the feature set in Task 2. Adding orthographic features (case, digit checks) and morphological features (suffixes, POS prefixes) provided the CRF with more discriminative information, leading to better identification of entities, especially PERSON and ORG. This is a classic lesson in traditional ML: feature quality heavily influences model performance.</li>
                <li><strong>Regularization Impact (Task 2 vs. Task 3):</strong> Task 3 demonstrates that overly strong L1 regularization (<code>c1=200</code>) can be detrimental. While L1 promotes sparsity (useful for high-dimensional feature spaces), too much can eliminate useful features, leading to underfitting and poor generalization, as seen by the drastic drop in F1 scores.</li>
                <li><strong>Transition Handling (Task 2 vs. Task 4):</strong> Setting <code>all_possible_transitions=True</code> in Task 4, which allows the model to learn weights for transitions not explicitly seen in training data, did not universally improve performance over Task 2 (where it was <code>False</code>). While it slightly helped I-PERSON, it severely hurt B-ORG. This suggests that for this dataset and feature set, allowing unseen transitions might have introduced noise or less reliable patterns for some entity types, or that the default transition probabilities learned from observed data in Task 2 were more robust.</li>
                <li><strong>Hyperparameter Optimization Challenges (Task 5):</strong> Task 5's HPO attempt yielded the worst results among tasks using enhanced features. This highlights that HPO is sensitive to the search space, number of iterations, cross-validation strategy, and the stability of the underlying training function. The poor result suggests that the HPO process might have converged to a suboptimal local minimum or that the base model configuration within the HPO (<code>task5_train_crf_model</code> with <code>all_possible_transitions=True</code>) was itself not as strong as the simpler <code>task1_train_crf_model</code> used in Task 2. The <code>UndefinedMetricWarning</code>s during HPO further point to potential instability with many hyperparameter combinations.</li>
            </ul>
            <p>In summary, for this CRF lab on NER, a rich feature set combined with moderate default regularization (as in Task 2) provided the best results. Aggressive regularization or changes to transition handling without careful tuning did not prove beneficial, and the HPO attempt was unsuccessful, possibly due to the search setup or inherent limitations of the training function used in that specific task.</p>
        </section>

        <section id="lab-crf-variations">
            <h2>2.2. CRF Variations</h2>
            <ul>
                <li><strong>Higher-Order CRFs:</strong> Extend dependencies beyond adjacent labels (e.g., $y_t$ depends on $y_{t-1}, y_{t-2}$). Captures more complex patterns but increases computational complexity.</li>
                <li><strong>BiLSTM-CRF:</strong> Popular hybrid. BiLSTM learns rich contextualized features, fed to a CRF layer that models label dependencies and predicts optimal label sequence. BiLSTM excels at automatic feature extraction; CRF layer enforces sequence constraints.</li>
                <li><strong>Dynamic CRFs (DCRFs) & Latent-Dynamic CRFs (LDCRFs):</strong> Advanced extensions incorporating latent variables or modeling more complex graphical structures. LDCRFs assume hidden state variables associated with labels. Training can be challenging.</li>
            </ul>
        </section>

        <section id="lab-crf-uses-limitations">
            <h2>2.3. Uses and Limitations of CRFs in NLP</h2>
            <h4>Uses (Recap):</h4>
            <p>NER, POS Tagging, Shallow Parsing, Bioinformatics, Text Segmentation, Information Extraction.</p>
            <h4>Limitations:</h4>
            <ul>
                <li><strong>Training Time:</strong> Can be computationally intensive and slow, especially with large datasets/features. Complexity can be quadratic in label set size and nearly quadratic in training sample size for linear chains.</li>
                <li><strong>Feature Engineering:</strong> Performance heavily depends on manually designed features (time-consuming, requires domain expertise). Key motivation for shift to deep learning.</li>
                <li><strong>Overfitting:</strong> Prone with many features if regularization is inadequate or features are noisy.</li>
                <li><strong>Complexity with Large Label/Feature Sets:</strong> Cost scales, potentially impractical without approximations.</li>
                <li><strong>Difficulty with Very Long-Range Dependencies:</strong> Standard linear-chain CRFs might struggle, as primary dependencies are local.</li>
            </ul>
            <p>Computational demands and feature engineering burden drove exploration of deep learning models (LSTMs, Transformers) which offer automatic feature learning.</p>
            <div class="note">
                <p><strong>Scalability (CRF Lab Insights):</strong> The lab experiments highlight that feature engineering is critical. While CRFs can handle many features, training time can be a bottleneck (max_iter=20 in labs). The choice of training algorithm and hyperparameters (Task 3 vs. Task 2, Task 5 results) significantly impacts performance and convergence, affecting practical scalability. For larger datasets than used in the lab, the computational cost of training a CRF with a rich feature set would be a major consideration.</p>
            </div>
        </section>

    </main>

    <footer class="bg-gray-800 text-white py-8 mt-12">
        <div class="container mx-auto px-6 text-center">
            <p>&copy; <span id="currentYear"></span> NLP Fundamentals & Labs. For educational purposes.</p>
            <p class="text-sm text-gray-400">Content derived from "NLP Exam Preparation" notes and "NLP Labs Detailed Notes".</p>
        </div>
    </footer>

    <script>
        // KaTeX auto-render
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ],
                throwOnError : false
            });

            // Set current year in footer
            document.getElementById('currentYear').textContent = new Date().getFullYear();

            // Active Nav Link Highlighting & Mobile Menu Toggle
            const currentLocation = window.location.pathname.split('/').pop() || 'index.html';
            const navLinks = document.querySelectorAll('nav a.nav-link');
            navLinks.forEach(link => {
                if (link.getAttribute('href') === currentLocation) {
                    link.classList.add('active');
                    link.classList.remove('text-gray-700');
                } else {
                    link.classList.remove('active');
                    link.classList.add('text-gray-700');
                }
            });

            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileMenu = document.getElementById('mobile-menu');
            if (mobileMenuButton && mobileMenu) {
                mobileMenuButton.addEventListener('click', function() {
                    mobileMenu.classList.toggle('hidden');
                });
            }
        });
    </script>

</body>
</html>
